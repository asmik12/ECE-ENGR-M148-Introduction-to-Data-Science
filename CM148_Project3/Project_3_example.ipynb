{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43cbc574",
   "metadata": {},
   "source": [
    "# Project 3 - Classify your own data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef714327",
   "metadata": {},
   "source": [
    "For this project we're going to explore some of the new topics since the last project including Decision Trees and Un-supervised learning. The final part of the project will ask you to perform your own data science project to classify a new dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59b8b12",
   "metadata": {},
   "source": [
    "## Loading Essentials and Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31567c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix for windows memory leak with MKL\n",
    "import os\n",
    "import platform\n",
    "\n",
    "if platform.system() == \"Windows\":\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d48374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt  # this is used for the plot the graph\n",
    "\n",
    "# Sklearn classes\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    "    KFold,\n",
    ")\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, silhouette_score\n",
    "import sklearn.metrics.cluster as smc\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    OneHotEncoder,\n",
    "    LabelEncoder,\n",
    "    MinMaxScaler,\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from helper import (\n",
    "    draw_confusion_matrix,\n",
    "    heatmap,\n",
    "    make_meshgrid,\n",
    "    plot_contours,\n",
    "    draw_contour,\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002d19e1",
   "metadata": {},
   "source": [
    "# Example Project using new techniques \n",
    "\n",
    "Since project 2, we have learned about a few new models for supervised learning(Decision Trees and Neural Networks) and un-supervised learning (Clustering and PCA). In this example portion, we will go over how to implement these techniques using the Sci-kit learn library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048640ad",
   "metadata": {},
   "source": [
    "## Load and Process Example Project Data\n",
    "\n",
    "\n",
    "For our example dataset, we will use the [Breast Cancer Wisconsin Dataset](https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data) to determine whether a mass found in a body is benign or malignant. Since this dataset was used as an example in project 2, you should be fairly familiar with it.\n",
    "\n",
    "Feature Information:\n",
    "\n",
    "Column 1: ID number\n",
    "\n",
    "Column 2: Diagnosis (M = malignant, B = benign)\n",
    "\n",
    "Ten real-valued features are computed for each cell nucleus:\n",
    "\n",
    "1. radius (mean of distances from center to points on the perimeter)\n",
    "2. texture (standard deviation of gray-scale values)\n",
    "3. perimeter\n",
    "4. area\n",
    "5. smoothness (local variation in radius lengths)\n",
    "6. compactness (perimeter^2 / area - 1.0)\n",
    "7. concavity (severity of concave portions of the contour)\n",
    "8. concave points (number of concave portions of the contour)\n",
    "9. symmetry\n",
    "10. fractal dimension (`coastline approximation` - 1)\n",
    "\n",
    "Due to the statistical nature of the test, we are not able to get exact measurements of the previous values. Instead, the dataset contains the mean and standard error of the real-valued features. \n",
    "\n",
    "Columns 3-12 present the mean of the measured values\n",
    "\n",
    "Columns 13-22 present the standard error of the measured values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817d6dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Data\n",
    "\n",
    "# Load Data\n",
    "data = pd.read_csv(\"datasets/breast_cancer_data.csv\")\n",
    "\n",
    "# Drop id column\n",
    "data = data.drop([\"id\"], axis=1)\n",
    "\n",
    "# Transform target feature into numerical\n",
    "le = LabelEncoder()\n",
    "data[\"diagnosis\"] = le.fit_transform(data[\"diagnosis\"])\n",
    "\n",
    "# Split target and data\n",
    "y = data[\"diagnosis\"]\n",
    "x = data.drop([\"diagnosis\"], axis=1)\n",
    "\n",
    "# Train test split\n",
    "train_raw, test_raw, target, target_test = train_test_split(\n",
    "    x, y, test_size=0.2, stratify=y, random_state=SEED\n",
    ")\n",
    "\n",
    "# Standardize data\n",
    "# Since all features are real-valued, we only have one pipeline\n",
    "pipeline = Pipeline([(\"scaler\", StandardScaler())])\n",
    "\n",
    "# Transform raw data\n",
    "train = pipeline.fit_transform(train_raw)\n",
    "test = pipeline.transform(test_raw)  # Note that there is no fit calls\n",
    "\n",
    "# Names of Features after Pipeline\n",
    "feature_names = list(pipeline.get_feature_names_out(list(x.columns)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a7cffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "target.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde7902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline accuracy of using the majority class\n",
    "ct = target_test.value_counts()\n",
    "print(\"Counts of each class in target_test: \")\n",
    "print(ct)\n",
    "print(\"Baseline Accuraccy of using Majority Class: \", np.max(ct) / np.sum(ct))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257a803a",
   "metadata": {},
   "source": [
    "## Supervised Learning: Decision Tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53754aaa",
   "metadata": {},
   "source": [
    "### Classification with Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f98cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=0)\n",
    "clf.fit(train, target)\n",
    "predicted = clf.predict(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5832b311",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%-12s %f\" % (\"Accuracy:\", metrics.accuracy_score(target_test, predicted)))\n",
    "print(\"Confusion Matrix: \\n\", metrics.confusion_matrix(target_test, predicted))\n",
    "draw_confusion_matrix(target_test, predicted, [\"healthy\", \"sick\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76d80c5",
   "metadata": {},
   "source": [
    "###  Parameters for Decision Tree Classifier\n",
    "\n",
    "In `scikit-learn`, the following are just some of the parameters we can pass into the Decision Tree Classifier:\n",
    "\n",
    "- `criterion`: {`gini`, `entropy`, `log_loss`} default=`gini`\n",
    "    - The function to measure the quality of a split. Supported criteria are `gini` for the Gini impurity and `log_loss` and `entropy` both for the Shannon information gain \n",
    "- `splitter`: {`best`, `random`}, default=`best`\n",
    "    - The strategy used to choose the split at each node. `best` aims to find the best feature split amongst all features. `random` only looks for the best split amongst a random subset of features.\n",
    "- `max_depth`: int, default = 2 {`newton-cg`, `lbfgs`, `liblinear`, `sag`, `saga`}, default=`lbfgs`\n",
    "    - The maximum depth of the tree.\n",
    "- `min_samples_split`: `int` or `float`, default=`2`\n",
    "    - The minimum number of samples required to split an internal node. If `int`, then consider `min_samples_split` as the minimum number. If float, then `min_samples_split` is a fraction and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decd1c24",
   "metadata": {},
   "source": [
    "### Visualizing Decision Trees\n",
    "\n",
    "Scikit-learn allows us to visualize the decision tree to see what features it choose to split and what the result is. Note that if the condition in the node is true, you traverse the left edge of the node. Otherwise, you traverse the right edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc6921a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 15))\n",
    "# Note that we have to pass the feature names into the plotting function to get the actual names\n",
    "# We pass the column names through the pipeline in case any feature augmentation was made\n",
    "# For example, a categorical feature will be split into multiple features with one hot encoding\n",
    "# and this way assigns a name to each column based on the feature value and the original feature name\n",
    "tree.plot_tree(\n",
    "    clf, max_depth=1, proportion=True, feature_names=feature_names, filled=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2565ef",
   "metadata": {},
   "source": [
    "We can even look at the tree in a textual format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350c60b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(export_text(clf, feature_names=feature_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2df556",
   "metadata": {},
   "source": [
    "### Feature Importance in Decision Trees\n",
    "\n",
    "Decision Trees can also assign importance to features by measuring the average decrease in impurity (i.e. information gain) for each feature. The features with higher decreases are treated as more important. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c695f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_pd = pd.Series(data=clf.feature_importances_, index=feature_names)\n",
    "imp_pd = imp_pd.sort_values(ascending=False)\n",
    "imp_pd.plot.bar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef272aec",
   "metadata": {},
   "source": [
    "We can clearly see that \"concave points_mean\" has the largest importance due to it providing the most reduction in the impurity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f59dbb",
   "metadata": {},
   "source": [
    "### Visualizing decision boundaries for Decision Trees\n",
    "\n",
    "Similar to project 2, lets see what decision boundaries that a Decision Tree creates. We use the two most correlated features to the target labels: concave_points_mean and perimeter_mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b5b0a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extract first two feature and use the standardscaler\n",
    "train_2 = StandardScaler().fit_transform(\n",
    "    train_raw[[\"concave points_mean\", \"perimeter_mean\"]]\n",
    ")\n",
    "\n",
    "depth = [1, 2, 3, 4, 5, 10, 15]\n",
    "for d in depth:\n",
    "    dt = DecisionTreeClassifier(max_depth=d, min_samples_split=7)\n",
    "    dt.fit(train_2, target)\n",
    "    draw_contour(train_2, target, dt, class_labels=[\"Benign\", \"Malignant\"])\n",
    "\n",
    "    plt.title(f\"Max Depth ={d}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730c6b55",
   "metadata": {},
   "source": [
    "We can see that the model gets more and more complex with increasing depth until it converges somewhere in between depth 10 and 15. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5060d5",
   "metadata": {},
   "source": [
    "## Supervised Learning: Multi-Layer Perceptron (MLP)\n",
    "\n",
    "A neural network is a series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates. Neural networks are very powerful tools that are used a in a variety of applications including image and speech processing. In class, we have discussed one of the earliest types of neural networks known as a Multi-Layer Perceptron. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb7df35",
   "metadata": {},
   "source": [
    "![steps](jupyter_images/mlp_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9697d8",
   "metadata": {},
   "source": [
    "### Using MLP for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9238c9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=SEED)\n",
    "clf.fit(train, target)\n",
    "predicted = clf.predict(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39a9e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%-12s %f\" % (\"Accuracy:\", metrics.accuracy_score(target_test, predicted)))\n",
    "print(\"Confusion Matrix: \\n\", metrics.confusion_matrix(target_test, predicted))\n",
    "draw_confusion_matrix(target_test, predicted, [\"Benign\", \"Malignant\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8703bb67",
   "metadata": {},
   "source": [
    "###  Parameters for MLP Classifier\n",
    "\n",
    "In `scikit-learn`, the following are just some of the parameters we can pass into MLP Classifier:\n",
    "\n",
    "- `hidden_layer_sizes`: `tuple`, length = n_layers - 2, default=(100,)\n",
    "    - The ith element represents the number of neurons in the ith hidden layer. \n",
    "- `activation`: {`identity`, `logistic`, `tanh`,`relu`}, default=`relu`\n",
    "    - Activation function for the hidden layer.\n",
    "- `alpha`: `float`, default = 0.0001\n",
    "    - Strength of the L2 regularization term. The L2 regularization term is divided by the sample size when added to the loss.\n",
    "- `max_iter`: `int`, default=200\n",
    "    - Maximum number of iterations taken for the solvers to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae499c7",
   "metadata": {},
   "source": [
    "### Visualizing decision boundaries for MLP\n",
    "\n",
    "Now, lets see how the decision boundaries change as a function of both the activation function and the number of hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453eae3c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Example of using the default Relu activation while altering the number of hidden layers\n",
    "train_2 = StandardScaler().fit_transform(\n",
    "    train_raw[[\"concave points_mean\", \"perimeter_mean\"]]\n",
    ")\n",
    "\n",
    "layers = [50, 100, 150, 200]\n",
    "for l in layers:\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(l,), max_iter=400)\n",
    "    mlp.fit(train_2, target)\n",
    "    draw_contour(train_2, target, mlp, class_labels=[\"Benign\", \"Malignant\"])\n",
    "    plt.title(f\"Hidden Layer Size ={l}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2984c343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using the default Relu activation\n",
    "# while altering the number of hidden layers with 2 groups of hidden layers\n",
    "train_2 = StandardScaler().fit_transform(\n",
    "    train_raw[[\"concave points_mean\", \"perimeter_mean\"]]\n",
    ")\n",
    "\n",
    "layers = [50, 100, 150, 200]\n",
    "for l in layers:\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(l, l), max_iter=400)\n",
    "    mlp.fit(train_2, target)\n",
    "    draw_contour(train_2, target, mlp, class_labels=[\"Benign\", \"Malignant\"])\n",
    "    plt.title(f\"Hidden Layer Sizes ={(l,l)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434d289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using 2 hidden layers of 100 units each with varying activations\n",
    "train_2 = StandardScaler().fit_transform(\n",
    "    train_raw[[\"concave points_mean\", \"perimeter_mean\"]]\n",
    ")\n",
    "\n",
    "acts = [\"identity\", \"logistic\", \"tanh\", \"relu\"]\n",
    "for act in acts:\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(100, 100), activation=act, max_iter=400)\n",
    "    mlp.fit(train_2, target)\n",
    "    draw_contour(train_2, target, mlp, class_labels=[\"Benign\", \"Malignant\"])\n",
    "\n",
    "    plt.title(f\"Activation = {act}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd847dd1",
   "metadata": {},
   "source": [
    "## Unsupervised learning: PCA\n",
    "\n",
    "As shown in lecture, PCA is a valuable dimensionality reduction tool that can extract a small subset of valuable features. In this section, we shall demonstrate how PCA can extract important visual features from pictures of subjects faces. We shall use the [AT&T Database of Faces](https://www.kaggle.com/datasets/kasikrit/att-database-of-faces). This dataset contains 40 different subjects with 10 samples per subject which means we a dataset of 400 samples. \n",
    "\n",
    "We extract the images from the [scikit-learn dataset library](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_olivetti_faces.html#sklearn.datasets.fetch_olivetti_faces). The library imports the images (faces.data), the flatten array of images (faces.images), and which subject eacj image belongs to (faces.target). Each image is a 64 by 64 image with pixels converted to floating point values in [0,1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327d360b",
   "metadata": {},
   "source": [
    "### Eigenfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096f817f",
   "metadata": {},
   "source": [
    "The following codes downloads and loads the face data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f441df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import faces from scikit library\n",
    "faces = datasets.fetch_olivetti_faces()\n",
    "print(\"Flattened Face Data shape:\", faces.data.shape)\n",
    "print(\"Face Image Data Shape:\", faces.images.shape)\n",
    "print(\"Shape of target data:\", faces.target.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf826c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract image shape for future use\n",
    "im_shape = faces.images[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95897a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints some example faces\n",
    "faceimages = faces.images[\n",
    "    np.random.choice(len(faces.images), size=16, replace=False)\n",
    "]  # take random 16 images\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, sharex=True, sharey=True, figsize=(8, 10))\n",
    "for i in range(16):\n",
    "    axes[i % 4][i // 4].imshow(faceimages[i], cmap=\"gray\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130ab664",
   "metadata": {},
   "source": [
    "Now, let us see what features we can extract from these face images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d54236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "pca_pipe = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"scaler\",\n",
    "            StandardScaler(),\n",
    "        ),  # Scikit learn PCA does not standardize so we need to add\n",
    "        (\"pca\", pca),\n",
    "    ]\n",
    ")\n",
    "pca_pipe.fit(faces.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bb570b",
   "metadata": {},
   "source": [
    "The following plots the top 30 PCA components with how much variance does this feature explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50543fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 6))\n",
    "for i in range(30):\n",
    "    ax = fig.add_subplot(3, 10, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(pca.components_[i].reshape(im_shape), cmap=plt.cm.bone)\n",
    "    ax.set_title(f\"Var={pca.explained_variance_ratio_[i]:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd6a2ec",
   "metadata": {},
   "source": [
    "Amazing! We can see that the model has learned to focus on many features that we as humans also look at when trying to identify a face such as the nose,eyes, eyebrows, etc.\n",
    "\n",
    "With this feature extraction, we can perform much more powerful learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b062142",
   "metadata": {},
   "source": [
    "### Feature Extraction for Classification\n",
    "\n",
    "Lets see if we can use PCA to improve the accuracy of the decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a25ff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without PCA\n",
    "clf = DecisionTreeClassifier(random_state=SEED)\n",
    "clf.fit(train, target)\n",
    "predicted = clf.predict(test)\n",
    "\n",
    "print(\"Accuracy without PCA\")\n",
    "print(\"%-12s %f\" % (\"Accuracy:\", metrics.accuracy_score(target_test, predicted)))\n",
    "print(\"Confusion Matrix: \\n\", metrics.confusion_matrix(target_test, predicted))\n",
    "draw_confusion_matrix(target_test, predicted, [\"Benign\", \"Malignant\"])\n",
    "\n",
    "# With PCA\n",
    "pca = PCA(n_components=0.9)  # Take components that explain at lest 90% variance\n",
    "\n",
    "train_new = pca.fit_transform(train)\n",
    "test_new = pca.transform(test)\n",
    "\n",
    "clf_pca = DecisionTreeClassifier(random_state=SEED)\n",
    "clf_pca.fit(train_new, target)\n",
    "predicted = clf_pca.predict(test_new)\n",
    "\n",
    "print(\"Accuracy with PCA\")\n",
    "print(\"%-12s %f\" % (\"Accuracy:\", metrics.accuracy_score(target_test, predicted)))\n",
    "print(\"Confusion Matrix: \\n\", metrics.confusion_matrix(target_test, predicted))\n",
    "draw_confusion_matrix(target_test, predicted, [\"Benign\", \"Malignant\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910e6b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Features without PCA: \", train.shape[1])\n",
    "print(\"Number of Features with PCA: \", train_new.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3522541a",
   "metadata": {},
   "source": [
    "Clearly, we get a much better accuracy for the model while using fewer features. But does the features the PCA thought were important the same features that the decision tree used. Lets look at the feature importance of the tree. The following plot numbers the first principal component as 0, the second as 1, and so forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc705e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_new = list(range(train_new.shape[1]))\n",
    "imp_pd = pd.Series(data=clf_pca.feature_importances_, index=feature_names_new)\n",
    "imp_pd = imp_pd.sort_values(ascending=False)\n",
    "imp_pd.plot.bar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f177147",
   "metadata": {},
   "source": [
    "Amazingly, the first and second components were the most important features in the decision tree. Thus, we can claim that PCA has significantly improved the performance of our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afa8402",
   "metadata": {},
   "source": [
    "## Unsupervised learning: Clustering\n",
    "\n",
    "Clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups. One major algorithm we learned in class is the K-Means algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1837030",
   "metadata": {},
   "source": [
    "### Evaluating K-Means performance\n",
    "\n",
    "While there are many ways to evaluate the [performance measure of clustering algorithsm](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation), we will focus on the inertia score of the K-Means model. Inertia is another term for the sum of squared distances of samples to their closest cluster center. \n",
    "\n",
    "Let us look at how the Inertia changes as a function of the number of clusters for an artificial dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4237ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artifical Dataset\n",
    "X, y = make_blobs(\n",
    "    n_samples=500,\n",
    "    n_features=2,\n",
    "    centers=5,\n",
    "    cluster_std=1,\n",
    "    center_box=(-10.0, 10.0),\n",
    "    shuffle=True,\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, edgecolor=\"k\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c190d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = list(range(2, 10))\n",
    "\n",
    "inertia = []\n",
    "for k in ks:\n",
    "    kmeans = KMeans(n_clusters=k, init=\"k-means++\", random_state=SEED)\n",
    "    kmeans.fit(X)\n",
    "    # inertia method returns wcss for that model\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    print(f\"Inertia for K = {k}: {kmeans.inertia_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75565ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(ks, inertia, marker=\"o\", color=\"red\")\n",
    "plt.title(\"The Elbow Method\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f5286f",
   "metadata": {},
   "source": [
    "From the plot, we can see that when the number of clusters of K-means is the correct number of clusters, Inertia starts decreasing at a much slower rate. This creates a kind of elbow shape in the graph. For K-means clustering, the elbow method selects the number of clusters where the elbow shape is formed. In this case, we see that this method would produce the correct number of clusters.\n",
    "\n",
    "Lets try it on the cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf62642",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = list(range(2, 30))\n",
    "inertia = []\n",
    "for k in ks:\n",
    "    kmeans = KMeans(n_clusters=k, init=\"k-means++\", random_state=SEED)\n",
    "    kmeans.fit(train)\n",
    "    # inertia method returns wcss for that model\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    print(f\"Inertia for K = {k}: {kmeans.inertia_:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd9ff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(ks, inertia, marker=\"o\", color=\"red\")\n",
    "plt.title(\"The Elbow Method\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46fc860",
   "metadata": {},
   "source": [
    "Here we see that the elbow is not as cleanly defined. This may be due to the dataset not being a good fit for K-means. Regardless, we can still apply the elbow method by noting that the slow down happens around 7~14."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035024b2",
   "metadata": {},
   "source": [
    "### Kmeans on Eigenfaces\n",
    "\n",
    "Now, lets see how K-means performs in clustering the face data with PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f283e35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 10  # We know there are 10 subjects\n",
    "km = KMeans(n_clusters=n_clusters, random_state=SEED)\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),  # First standardize\n",
    "        (\"pca\", PCA()),  # Transform using pca\n",
    "        (\"kmeans\", km),\n",
    "    ]\n",
    ")  # Then apply k means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2937f475",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = pipe.fit_predict(faces.data)\n",
    "print(clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6f4ebf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for labelID in range(n_clusters):\n",
    "    # find all indexes into the `data` array that belong to the\n",
    "    # current label ID, then randomly sample a maximum of 25 indexes\n",
    "    # from the set\n",
    "    idxs = np.where(clusters == labelID)[0]\n",
    "    idxs = np.random.choice(idxs, size=min(25, len(idxs)), replace=False)\n",
    "\n",
    "    # Extract the sampled indexes\n",
    "    id_face = faces.images[idxs]\n",
    "\n",
    "    # Plots sampled faces\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    for i in range(min(25, len(idxs))):\n",
    "        ax = fig.add_subplot(5, 5, i + 1, xticks=[], yticks=[])\n",
    "        ax.imshow(id_face[i], cmap=plt.cm.bone)\n",
    "    fig.suptitle(f\"Id={labelID}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db5d278",
   "metadata": {},
   "source": [
    "While the algorithm isn't perfect, we can see that K-means with PCA is picking up on some facial similarity or similar expressions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "429px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
