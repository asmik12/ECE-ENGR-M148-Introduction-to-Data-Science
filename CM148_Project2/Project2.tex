\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro

    \usepackage{iftex}
    \ifPDFTeX
        \usepackage[T1]{fontenc}
        \IfFileExists{alphabeta.sty}{
              \usepackage{alphabeta}
          }{
              \usepackage[mathletters]{ucs}
              \usepackage[utf8x]{inputenc}
          }
    \else
        \usepackage{fontspec}
        \usepackage{unicode-math}
    \fi

    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{array}     % table support for pandoc >= 2.11.3
    \usepackage{calc}      % table minipage width calculation for pandoc >= 2.11.1
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{Project2}
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@ges}{\let\PY@bf=\textbf\let\PY@it=\textit}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb.
    \makeatletter
        \newbox\Wrappedcontinuationbox
        \newbox\Wrappedvisiblespacebox
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}}
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}}
        \newcommand*\Wrappedcontinuationindent {3ex }
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox}
        % Take advantage of the already applied Pygments mark-up to insert
        % potential linebreaks for TeX processing.
        %        {, <, #, %, $, ' and ": go to next line.
        %        _, }, ^, &, >, - and ~: stay at end of broken line.
        % Use of \textquotesingle for straight quote.
        \newcommand*\Wrappedbreaksatspecials {%
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}%
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}%
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}%
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}%
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}%
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}%
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}%
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}%
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}%
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}%
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}%
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}%
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}%
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}%
        }
        % Some characters . , ; ? ! / are not pygmentized.
        % This macro makes them "active" and they will insert potential linebreaks
        \newcommand*\Wrappedbreaksatpunct {%
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}%
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}%
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}%
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}%
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}%
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}%
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}%
            \catcode`\.\active
            \catcode`\,\active
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active
            \lccode`\~`\~
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%

        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}

    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \hypertarget{project-2---binary-classification-comparative-methods}{%
\section{Project 2 - Binary Classification Comparative
Methods}\label{project-2---binary-classification-comparative-methods}}

    For this project we're going to attempt a binary classification of a
dataset using multiple methods and compare results.

Our goals for this project will be to introduce you to several of the
most common classification techniques, how to perform them and tweek
parameters to optimize outcomes, how to produce and interpret results,
and compare performance. You will be asked to analyze your findings and
provide explanations for observed performance.

DEFINITIONS

Binary Classification: In this case a complex dataset has an added
`target' label with one of two options. Your learning algorithm will try
to assign one of these labels to the data.

Supervised Learning: This data is fully supervised, which means it's
been fully labeled and we can trust the veracity of the labeling.

    \hypertarget{submission-details}{%
\subsection{Submission Details}\label{submission-details}}

\textbf{Project is due May 15th at 12:00 pm (Sunday Midnight). To submit
the project, please save the notebook as a pdf file and submit the
assignment via Gradescope. In addition, make sure that all figures are
legible and suﬀiciently large. For best pdf results, we recommend print
the notebook using \href{https://www.latex-project.org/}{\(\LaTeX\)}.}

    \hypertarget{loading-essentials-and-helper-functions}{%
\subsection{Loading Essentials and Helper
Functions}\label{loading-essentials-and-helper-functions}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Here are a set of libraries we imported to complete this assignment.}
\PY{c+c1}{\PYZsh{}Feel free to use these or equivalent libraries for your implementation}
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np} \PY{c+c1}{\PYZsh{} linear algebra}
\PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd} \PY{c+c1}{\PYZsh{} data processing, CSV file I/O (e.g. pd.read\PYZus{}csv)}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt} \PY{c+c1}{\PYZsh{} this is used for the plot the graph}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}
\PY{k+kn}{import} \PY{n+nn}{os}
\PY{k+kn}{import} \PY{n+nn}{time}
\PY{c+c1}{\PYZsh{}Sklearn classes}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k+kn}{import} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{,} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{,} \PY{n}{GridSearchCV}\PY{p}{,} \PY{n}{KFold}
\PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k+kn}{import} \PY{n}{metrics}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{svm} \PY{k+kn}{import} \PY{n}{SVC}  \PY{c+c1}{\PYZsh{}SVM classifier}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k+kn}{import} \PY{n}{LogisticRegression}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{neighbors} \PY{k+kn}{import} \PY{n}{KNeighborsClassifier}

\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k+kn}{import} \PY{n}{confusion\PYZus{}matrix}
\PY{k+kn}{import} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics}\PY{n+nn}{.}\PY{n+nn}{cluster} \PY{k}{as} \PY{n+nn}{smc}

\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{pipeline} \PY{k+kn}{import} \PY{n}{Pipeline}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k+kn}{import} \PY{n}{StandardScaler}\PY{p}{,} \PY{n}{OneHotEncoder}\PY{p}{,} \PY{n}{Normalizer}\PY{p}{,} \PY{n}{MinMaxScaler}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{compose} \PY{k+kn}{import} \PY{n}{ColumnTransformer}\PY{p}{,} \PY{n}{make\PYZus{}column\PYZus{}transformer}

\PY{k+kn}{from} \PY{n+nn}{matplotlib} \PY{k+kn}{import} \PY{n}{pyplot} \PY{k}{as} \PY{n}{plt}

\PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline

\PY{c+c1}{\PYZsh{}Sets random seed}
\PY{k+kn}{import} \PY{n+nn}{random}
\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{42}\PY{p}{)}

\PY{c+c1}{\PYZsh{}Connecting to google drive}
\PY{k+kn}{from} \PY{n+nn}{google}\PY{n+nn}{.}\PY{n+nn}{colab} \PY{k+kn}{import} \PY{n}{drive}
\PY{n}{drive}\PY{o}{.}\PY{n}{mount}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{/content/drive}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{k+kn}{import} \PY{n+nn}{sys}
\PY{n}{sys}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{/content/drive/MyDrive/CM148\PYZus{}Project2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{k+kn}{import} \PY{n+nn}{helper} \PY{k}{as} \PY{n+nn}{helper}

\PY{c+c1}{\PYZsh{} import the provided helper functions}
\PY{c+c1}{\PYZsh{}from helper import save\PYZus{}fig, draw\PYZus{}confusion\PYZus{}matrix, heatmap, make\PYZus{}meshgrid, plot\PYZus{}contours}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Mounted at /content/drive
    \end{Verbatim}

    \hypertarget{project-using-classification-methods-to-classify-heart-disease}{%
\section{Project: Using classification methods to classify heart
disease}\label{project-using-classification-methods-to-classify-heart-disease}}

Now that you have some examples of the classifiers that Sci-kit learn
has to offers, let try to apply them to a new dataset.

    \hypertarget{background-the-dataset}{%
\subsection{Background: The Dataset}\label{background-the-dataset}}

    For this exercise we will be using a subset of the UCI Heart Disease
dataset, leveraging the fourteen most commonly used attributes. All
identifying information about the patient has been scrubbed. You will be
asked to classify whether a patient is suffering from heart disease
based on a host of potential medical factors.

The dataset includes 14 columns. The information provided by each column
is as follows:

age: Age in years

sex: Male / Female

cp: Chest pain type (0 = asymptomatic; 1 = atypical angina; 2 =
non-anginal pain; 3 = typical angina)

trestbps: Resting blood pressure (in mm Hg on admission to the hospital)

chol: cholesterol in mg/dl

fbs Fasting blood sugar \textgreater{} 120 mg/dl (1 = true; 0 = false)

restecg: Resting electrocardiographic results (0= showing probable or
definite left ventricular hypertrophy by Estes' criteria; 1 = normal; 2
= having ST-T wave abnormality (T wave inversions and/or ST elevation or
depression of \textgreater{} 0.05 mV))

thalach: Maximum heart rate achieved

exang: Exercise induced angina (1 = yes; 0 = no)

oldpeak: Depression induced by exercise relative to rest

slope: The slope of the peak exercise ST segment (0 = downsloping; 1 =
flat; 2 = upsloping)

ca: Number of major vessels (0-3) colored by flourosopy

thal: 1 = normal; 2 = fixed defect; 7 = reversable defect

sick: Indicates the presence of Heart disease (True = Disease; False =
No disease)

    \hypertarget{pts-part-1.-load-the-data-and-analyze}{%
\subsection{{[}25 pts{]} Part 1. Load the Data and
Analyze}\label{pts-part-1.-load-the-data-and-analyze}}

    Let's first load our dataset so we'll be able to work with it. (correct
the relative path if your notebook is in a different directory than the
csv file.)

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}TODO: Your code goes here}
\PY{n}{data} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{/content/drive/MyDrive/CM148\PYZus{}Project2/datasets/heartdisease.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{pts-1.1-looking-at-the-data}{%
\subsubsection{{[}5 pts{]} 1.1 Looking at the
data}\label{pts-1.1-looking-at-the-data}}

Now that our data is loaded, let's take a closer look at the dataset
we're working with. Use the head method, the describe method, and the
info method to display some of the rows so we can visualize the types of
data fields we'll be working with.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}TODO: Your code goes here}
\PY{n}{data}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
   age     sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \textbackslash{}
0   63    Male   3       145   233    1        0      150      0      2.3
1   37    Male   2       130   250    0        1      187      0      3.5
2   41  Female   1       130   204    0        0      172      0      1.4
3   56    Male   1       120   236    0        1      178      0      0.8
4   57  Female   0       120   354    0        1      163      1      0.6

   slope  ca  thal   sick
0      0   0     1  False
1      0   0     2  False
2      2   0     2  False
3      2   0     2  False
4      2   0     2  False
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{data}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
              age          cp    trestbps        chol         fbs     restecg  \textbackslash{}
count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000
mean    54.366337    0.966997  131.623762  246.264026    0.148515    0.528053
std      9.082101    1.032052   17.538143   51.830751    0.356198    0.525860
min     29.000000    0.000000   94.000000  126.000000    0.000000    0.000000
25\%     47.500000    0.000000  120.000000  211.000000    0.000000    0.000000
50\%     55.000000    1.000000  130.000000  240.000000    0.000000    1.000000
75\%     61.000000    2.000000  140.000000  274.500000    0.000000    1.000000
max     77.000000    3.000000  200.000000  564.000000    1.000000    2.000000

          thalach       exang     oldpeak       slope          ca        thal
count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000
mean   149.646865    0.326733    1.039604    1.399340    0.729373    2.313531
std     22.905161    0.469794    1.161075    0.616226    1.022606    0.612277
min     71.000000    0.000000    0.000000    0.000000    0.000000    0.000000
25\%    133.500000    0.000000    0.000000    1.000000    0.000000    2.000000
50\%    153.000000    0.000000    0.800000    1.000000    0.000000    2.000000
75\%    166.000000    1.000000    1.600000    2.000000    1.000000    3.000000
max    202.000000    1.000000    6.200000    2.000000    4.000000    3.000000
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{data}\PY{o}{.}\PY{n}{info}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 303 entries, 0 to 302
Data columns (total 14 columns):
 \#   Column    Non-Null Count  Dtype
---  ------    --------------  -----
 0   age       303 non-null    int64
 1   sex       303 non-null    object
 2   cp        303 non-null    int64
 3   trestbps  303 non-null    int64
 4   chol      303 non-null    int64
 5   fbs       303 non-null    int64
 6   restecg   303 non-null    int64
 7   thalach   303 non-null    int64
 8   exang     303 non-null    int64
 9   oldpeak   303 non-null    float64
 10  slope     303 non-null    int64
 11  ca        303 non-null    int64
 12  thal      303 non-null    int64
 13  sick      303 non-null    bool
dtypes: bool(1), float64(1), int64(11), object(1)
memory usage: 31.2+ KB
    \end{Verbatim}

    Sometimes data will be stored in different formats (e.g., string, date,
boolean), but many learning methods work strictly on numeric inputs.
Additionally, some numerical features can represent categorical features
which need to be pre-processed. \textbf{Are there any columns that need
to be transformed and why?}

    {[}Use this area to describe any fields you believe will be problematic
and why{]} E.g., All the columns in our dataframe are numeric (either
int or float), however our target variable `sick' is a boolean and may
need to be modified.

    \textbf{Answer:} The `sex' column is populated with string responses
like `Male' or `Female' which might need to be problematic if passed
into the model without being one-hot encoded first since many learning
methods work strictly on numeric inputs. Since all the other fields
(except the target field) are numeric, they are unlikely to pose
problems unless the magnitude of any of their values is very low or very
high, causing underflow/overflow errors. As mentioned in the explanation
above, the target `sick' might also need to be modified into a numeric
form (e.g 1 for True, 0 for False) to be compared to the numeric output
of a model.

    \textbf{Determine if we're dealing with any null values. If so, report
on which columns?}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}TODO: Your code goes here}
\PY{n}{data}\PY{o}{.}\PY{n}{isnull}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
age         0
sex         0
cp          0
trestbps    0
chol        0
fbs         0
restecg     0
thalach     0
exang       0
oldpeak     0
slope       0
ca          0
thal        0
sick        0
dtype: int64
\end{Verbatim}
\end{tcolorbox}
        
    \textbf{Answer:} As shown by the output above, we are not dealing with
any null values on any columns. This is also shown by
\texttt{data.info()} where there are 303 entries and all columns have
303 non-null values.

    \hypertarget{pts-1.2-transform-target-label-into-numerical-value}{%
\subsubsection{{[}5 pts{]} 1.2 Transform target label into numerical
value}\label{pts-1.2-transform-target-label-into-numerical-value}}

Before we begin our analysis, we need to fix the field(s) that will be
problematic. Specifically, \textbf{convert any categorical value into
binary/numerical value using the
\href{https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html}{label
encoder from scikit-learn}, place this new array into a new column of
the DataFrame named ``target'', and then drop the original columns from
the dataframe. Afterward, use .head to print the first 5 rows}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}TODO: Your code goes here}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k+kn}{import} \PY{n}{LabelEncoder}
\PY{n}{le} \PY{o}{=} \PY{n}{LabelEncoder}\PY{p}{(}\PY{p}{)}

\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{le}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sick}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sex}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{le}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sex}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\PY{n}{data} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sick}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)} \PY{c+c1}{\PYZsh{}dropping the original column from the dataset}
\PY{n}{data}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \textbackslash{}
0   63    1   3       145   233    1        0      150      0      2.3      0
1   37    1   2       130   250    0        1      187      0      3.5      0
2   41    0   1       130   204    0        0      172      0      1.4      2
3   56    1   1       120   236    0        1      178      0      0.8      2
4   57    0   0       120   354    0        1      163      1      0.6      2

   ca  thal  target
0   0     1       0
1   0     2       0
2   0     2       0
3   0     2       0
4   0     2       0
\end{Verbatim}
\end{tcolorbox}
        
    \hypertarget{pts-1.3-plotting-histogram-of-data}{%
\subsubsection{{[}5 pts{]} 1.3 Plotting histogram of
data}\label{pts-1.3-plotting-histogram-of-data}}

Now that we have a feel for the data-types for each of the variables,
plot histograms of each field.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}TODO: Your code goes here}
\PY{n}{data}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Project2_files/Project2_24_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{pts-1.4-looking-at-class-balance}{%
\subsubsection{{[}5 pts{]} 1.4 Looking at class
balance}\label{pts-1.4-looking-at-class-balance}}

We also want to make sure we are dealing with a balanced dataset. In
this case, we want to confirm whether or not we have an equitable number
of sick and healthy individuals to ensure that our classifier will have
a sufficiently balanced dataset to adequately classify the two.
\emph{Plot a histogram specifically of the sick target, and conduct a
count of the number of sick and healthy individuals and discuss the
results}:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}TODO: Your code goes here}
\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}

\PY{n}{count} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{count}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Project2_files/Project2_26_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
target
0    165
1    138
Name: count, dtype: int64
    \end{Verbatim}

    \textbf{Answer:} This dataset has fewer samples of sick people (138)
than of healthy people (165). This might result in a bias towards the
majority class or poorer performance in predictions of the minority
class (in this case the sick people). This means that the final model
might be more inclined to predict that a person is healthy than sick.

    Balanced datasets are important to ensure that classifiers train
adequately and don't overfit, however arbitrary balancing of a dataset
might introduce its own issues.

\textbf{Discuss some of the problems that might arise by artificially
balancing a dataset.}

\textbf{Answer:} Reducing the number of majority class samples to
balance the dataset can lead to the loss of valuable information,
potentially discarding important patterns. Conversely, over-sampling
(duplication) of minority class samples could cause the model to overfit
to these samples, recognizing the individual sample rather than learning
the underlying distribution. Artificially balancing the distribution can
also alter the natural distribution and relationships between classes.
For instance, if people are generally more likely to be healthy than
sick, then artificially balancing the dataset might result in the model
being equally likely to predict someone is sick vs if they are healthy,
which would not be representative of the true nature of the
distribution. As such, a bias might be introduced where minority class
samples are overrepresented while others are underrepresented.

    \hypertarget{pts-1.5-looking-at-data-correlation}{%
\subsubsection{{[}5 pts{]} 1.5 Looking at Data
Correlation}\label{pts-1.5-looking-at-data-correlation}}

Now that we have our dataframe prepared let's start analyzing our data.
For this next question let's look at the correlations of our variables
to our target value. First, use the heatmap function to plot the
correlations of the data.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}TODO: Your code goes here}
\PY{n}{correlations} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{p}{)}
\PY{n}{columns} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{data}\PY{p}{)}
\PY{c+c1}{\PYZsh{}Creating heatmap:}
\PY{n}{helper}\PY{o}{.}\PY{n}{heatmap}\PY{p}{(}\PY{n}{correlations}\PY{o}{.}\PY{n}{values}\PY{p}{,} \PY{n}{columns}\PY{p}{,} \PY{n}{columns}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{12}\PY{p}{)}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{hsv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Project2_files/Project2_30_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Next, show the correlation to the \texttt{target} feature only and sort
them in descending order.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}TODO: Your code goes here}
\PY{n}{correlations}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{target}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{.}\PY{n}{sort\PYZus{}values}\PY{p}{(}\PY{n}{ascending}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
target      1.000000
exang       0.436757
oldpeak     0.430696
ca          0.391724
thal        0.344029
sex         0.280937
age         0.225439
trestbps    0.144931
chol        0.085239
fbs         0.028046
restecg    -0.137230
slope      -0.345877
thalach    -0.421741
cp         -0.433798
Name: target, dtype: float64
\end{Verbatim}
\end{tcolorbox}
        
    \textbf{From the heatmap values and the description of the features, why
do you think some variables correlate more highly than others?} (This
question is just to get you thinking and there is no perfect answer
since we have no medical background)

    \textbf{Answer:} Some variables which are actually symptoms of the heart
disease or are closely related to/influenced by these symptoms would
have a high correlation to the existence of the heart disease, while
those that have no connection to heart disease would have low
correlation

    \hypertarget{pts-part-2.-prepare-the-data-and-run-a-knn-model}{%
\subsection{{[}25 pts{]} Part 2. Prepare the Data and run a KNN
Model}\label{pts-part-2.-prepare-the-data-and-run-a-knn-model}}

    Before running our various learning methods, we need to do some
additional prep to finalize our data. Specifically you'll have to cut
the classification target from the data that will be used to classify,
and then you'll have to divide the dataset into training and testing
cohorts.

Specifically, we're going to ask you to prepare 2 batches of data. The
first batch will simply be the raw numeric data that hasn't gone through
any additional pre-processing. The second batch will be data that you
will pipeline using pre-processing methods. We will then feed both of
these datasets into a classifier to showcase just how important this
step can be!

    \hypertarget{pts-2.1-separate-target-labels-from-data}{%
\subsubsection{{[}2 pts{]} 2.1 Separate target labels from
data}\label{pts-2.1-separate-target-labels-from-data}}

Save the label column as a separate array and then make a new dataframe
without the target.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}TODO: Your code goes here}
\PY{n}{y} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{c+c1}{\PYZsh{}label column}
\PY{n}{x} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{target}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)} \PY{c+c1}{\PYZsh{}new dataframe without target}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{pts-2.2-balanced-train-test-split}{%
\subsubsection{{[}5 pts{]} 2.2 Balanced Train Test
Split}\label{pts-2.2-balanced-train-test-split}}

Now, create your `Raw' unprocessed training data by dividing your
dataframe into training and testing cohorts, with your training cohort
consisting of 60\% of your total dataframe. To ensure that the train and
test sets have balanced classes, use the
\href{https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html}{stratify
command of train\_test\_split}. Output the resulting shapes of your
training and testing samples to confirm that your split was successful.
Additionally, output the class counts for the training and testing
cohorts to confirm that there is no artifical class imbalance.

Note: Use \texttt{randomstate\ =\ 0} to ensure that the same train/test
split happens everytime for ease of grading.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}TODO: Your code goes here}
\PY{n}{train\PYZus{}raw}\PY{p}{,} \PY{n}{test\PYZus{}raw}\PY{p}{,} \PY{n}{train\PYZus{}target}\PY{p}{,} \PY{n}{test\PYZus{}target} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.4}\PY{p}{,} \PY{n}{stratify}\PY{o}{=} \PY{n}{y}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{pts-2.3-knn-on-raw-data}{%
\subsubsection{{[}5 pts{]} 2.3 KNN on raw
data}\label{pts-2.3-knn-on-raw-data}}

Now, let's try a classification model on this data. We'll first use KNN
since it is the one we are most familiar with.

One thing we noted in class was that because KNN relies on Euclidean
distance, it is highly sensitive to the relative magnitude of different
features. Let's see that in action! Implement a K-Nearest Neighbor
algorithm on our data and report the results. For this initial
implementation, simply use the default settings. Refer to the
\href{https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html}{KNN
Documentation} for details on implementation. \textbf{Report on the test
accuracy of the resulting model and plot the confusion matrix.}

Recall that accurracy can be calculated easily using
\texttt{metrics.accuracy\_score} and that we have a helper function to
draw the confusion matrix.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}TODO: Your code goes here}
\PY{n}{knn} \PY{o}{=} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{p}{)}
\PY{n}{knn}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}raw}\PY{p}{,} \PY{n}{train\PYZus{}target}\PY{p}{)}
\PY{n}{prediction} \PY{o}{=} \PY{n}{knn}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{test\PYZus{}raw}\PY{p}{)} \PY{c+c1}{\PYZsh{}computing predictions on testing data}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZpc{}\PYZhy{}12s}\PY{l+s+s2}{ }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{.}\PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{test\PYZus{}target}\PY{p}{,}\PY{n}{prediction}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy:    0.655738
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{15}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{helper}\PY{o}{.}\PY{n}{draw\PYZus{}confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{test\PYZus{}target}\PY{p}{,} \PY{n}{prediction}\PY{p}{,} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Not Sick (0)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Sick (1)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Project2_files/Project2_43_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{pts-2.4-knn-on-preprocessed-data}{%
\subsubsection{{[}5 pts{]} 2.4 KNN on preprocessed
data}\label{pts-2.4-knn-on-preprocessed-data}}

Now lets implement a pipeline to preprocess the data. For the pipeline,
use \texttt{StandardScaler} on the numerical features and one-hot
encoding on the categorical features. For reference on how to make a
pipeline, please look at project 1.

For reference, the categorical features are
\texttt{{[}\textquotesingle{}sex\textquotesingle{},\ \textquotesingle{}cp\textquotesingle{},\ \textquotesingle{}fbs\textquotesingle{},\ \textquotesingle{}restecg\textquotesingle{},\ \textquotesingle{}exang\textquotesingle{},\ \textquotesingle{}slope\textquotesingle{},\ \textquotesingle{}ca\textquotesingle{},\textquotesingle{}thal\textquotesingle{}{]}}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{16}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}TODO: Your code goes here}
\PY{n}{cat\PYZus{}features} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sex}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cp}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{fbs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{restecg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{exang}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{slope}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ca}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{thal}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\PY{n}{num\PYZus{}features} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{trestbps}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{chol}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{thalach}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{oldpeak}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}

\PY{n}{cat\PYZus{}pipeline} \PY{o}{=} \PY{n}{Pipeline}\PY{p}{(}\PY{p}{[}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{onehot}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{OneHotEncoder}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{]}\PY{p}{)}
\PY{n}{num\PYZus{}pipeline} \PY{o}{=} \PY{n}{Pipeline}\PY{p}{(}\PY{p}{[}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{scaler}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{]}\PY{p}{)}

\PY{n}{full\PYZus{}pipeline} \PY{o}{=} \PY{n}{ColumnTransformer}\PY{p}{(}\PY{p}{[}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cat}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{cat\PYZus{}pipeline}\PY{p}{,} \PY{n}{cat\PYZus{}features}\PY{p}{)}\PY{p}{,}
                                   \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{num}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{num\PYZus{}pipeline}\PY{p}{,} \PY{n}{num\PYZus{}features}\PY{p}{)}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \textbf{Now use the pipeline to transform the data and then apply the
same KNN classifier with this new training/testing data. Report the test
accuracy. Discuss the implications of the different results you are
obtaining.}

Note: Remember to use \texttt{fit\_transform} on the training data and
\texttt{transform} on the testing data.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{17}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}TODO: Your code goes here}
\PY{n}{pipeline} \PY{o}{=} \PY{n}{Pipeline}\PY{p}{(}\PY{p}{[}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{scaler}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{]}\PY{p}{)}
\PY{n}{cat\PYZus{}pipeline} \PY{o}{=} \PY{n}{Pipeline}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cat}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{OneHotEncoder}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{cat\PYZus{}features}\PY{p}{]}\PY{p}{)}
\PY{n}{train\PYZus{}processed} \PY{o}{=} \PY{n}{full\PYZus{}pipeline}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{train\PYZus{}raw}\PY{p}{)}
\PY{n}{test\PYZus{}processed} \PY{o}{=} \PY{n}{full\PYZus{}pipeline}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{test\PYZus{}raw}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{18}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{knn} \PY{o}{=} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{p}{)}
\PY{n}{knn}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}processed}\PY{p}{,} \PY{n}{train\PYZus{}target}\PY{p}{)}
\PY{n}{prediction} \PY{o}{=} \PY{n}{knn}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{test\PYZus{}processed}\PY{p}{)}
\PY{n}{acc} \PY{o}{=} \PY{n}{metrics}\PY{o}{.}\PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{test\PYZus{}target}\PY{p}{,}\PY{n}{prediction}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZpc{}\PYZhy{}12s}\PY{l+s+s2}{ }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{acc}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy:    0.754098
    \end{Verbatim}

    \textbf{Answer:} Accuracy of the processed data is significantly higher
(0.762295) compared to the accuracy of the model on the raw data
(0.655738). This shows the importance of pre-processing to remove the
impacts of outliers, to normalize data and reduce the skewness of
results due to units of measurement/data values etc.

    \hypertarget{pts-2.5-knn-parameter-optimization-for-n_neighbors}{%
\subsubsection{\texorpdfstring{{[}8 pts{]} 2.5 KNN Parameter
optimization for
\texttt{n\_neighbors}}{{[}8 pts{]} 2.5 KNN Parameter optimization for n\_neighbors}}\label{pts-2.5-knn-parameter-optimization-for-n_neighbors}}

The KNN Algorithm includes an \texttt{n\_neighbors} attribute that
specifies how many neighbors to use when developing the cluster. (The
default value is 5, which is what your previous model used.)
\textbf{Let's now try n values of: 1, 2, 4, 8, 16, 32, 64. Run your
model for each value and report the test accuracy for each}.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{19}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}TODO: Your code goes here}
\PY{n}{n\PYZus{}values} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{16}\PY{p}{,}\PY{l+m+mi}{32}\PY{p}{,}\PY{l+m+mi}{64}\PY{p}{]}
\PY{n}{accuracy} \PY{o}{=} \PY{p}{[}\PY{p}{]}

\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{n\PYZus{}values}\PY{p}{:}
  \PY{n}{knn} \PY{o}{=} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{n}{n\PYZus{}neighbors}\PY{o}{=}\PY{n}{i}\PY{p}{)}
  \PY{n}{knn}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}processed}\PY{p}{,} \PY{n}{train\PYZus{}target}\PY{p}{)}
  \PY{n}{prediction} \PY{o}{=} \PY{n}{knn}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{test\PYZus{}processed}\PY{p}{)} \PY{c+c1}{\PYZsh{}computing predictions on testing data}
  \PY{n}{acc} \PY{o}{=} \PY{n}{metrics}\PY{o}{.}\PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{test\PYZus{}target}\PY{p}{,}\PY{n}{prediction}\PY{p}{)}
  \PY{n}{accuracy}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{acc}\PY{p}{)} \PY{c+c1}{\PYZsh{}keeping track of the accuracy}
  \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{N=}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{i}\PY{p}{)}
  \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZpc{}\PYZhy{}12s}\PY{l+s+s2}{ }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{acc}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
N= 1
Accuracy:    0.770492
N= 2
Accuracy:    0.745902
N= 4
Accuracy:    0.754098
N= 8
Accuracy:    0.762295
N= 16
Accuracy:    0.770492
N= 32
Accuracy:    0.778689
N= 64
Accuracy:    0.778689
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{20}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{n\PYZus{}values}\PY{p}{,} \PY{n}{accuracy}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{KNN: Neighbors vs Accuracy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{N values}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Project2_files/Project2_52_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \textbf{Comment for which value of n did the KNN model perform the best.
Did the model perform strictly better or strictly worse as the value of
n increased?}

    \textbf{Answer:} As shown by the graph, the model seems to perform
strictly better as the value of n increases over this specified range.
This means that for n = 32,64 the model performed the best. I suspect
that this might be the expected peak region of the graph of n\_values vs
accuracy, and that if I continue to plot accuracy vs n values all the
way up to 181 (number of samples), then the graph might resemble the
inverse U curve shape that is expected. Also, the datapoints are most
likely pretty far away from each other, and the points representing
sick/healthy people are clustered close together in clusters that are
pretty far apart. This might explain why even for larger values of k,
the accuracy remains high.

    So we have a model that seems to work well. But let's see if we can do
better! To do so we'll employ Logistic Regression and SVM to improve
upon the model and compare the results.

\textbf{For the rest of the project, you will only be using the
transformed data and not the raw data. DO NOT USE THE RAW DATA ANYMORE}

    \hypertarget{pts-part-3.-additional-learning-methods-logistic-regression}{%
\subsection{{[}20 pts{]} Part 3. Additional Learning Methods: Logistic
Regression}\label{pts-part-3.-additional-learning-methods-logistic-regression}}

    Let's now try Logistic Regression. Recall that Logistic regression is a
statistical model that in its basic form uses a logistic function to
model a binary dependent variable.

    \hypertarget{pts-3.1-run-the-default-logistic-regression}{%
\subsubsection{{[}5 pts{]} 3.1 Run the default Logistic
Regression}\label{pts-3.1-run-the-default-logistic-regression}}

Implement a Logistical Regression Classifier. Review the
\href{https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html}{Logistical
Regression Documentation} for how to implement the model. Use the
default settings. \textbf{Report on the test accuracy and plot the
confusion matrix.}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{21}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}TODO: Your code goes here}
\PY{n}{log\PYZus{}reg} \PY{o}{=} \PY{n}{LogisticRegression}\PY{p}{(}\PY{p}{)}
\PY{n}{log\PYZus{}reg}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}processed}\PY{p}{,} \PY{n}{train\PYZus{}target}\PY{p}{)}
\PY{n}{testing\PYZus{}result} \PY{o}{=} \PY{n}{log\PYZus{}reg}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{test\PYZus{}processed}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZpc{}\PYZhy{}12s}\PY{l+s+s2}{ }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{.}\PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{test\PYZus{}target}\PY{p}{,} \PY{n}{testing\PYZus{}result}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy:    0.819672
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{22}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{helper}\PY{o}{.}\PY{n}{draw\PYZus{}confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{test\PYZus{}target}\PY{p}{,} \PY{n}{testing\PYZus{}result}\PY{p}{,} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Not Sick (0)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Sick (1)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Project2_files/Project2_60_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{pts-3.2-compare-logistic-regression-and-knn}{%
\subsubsection{{[}5 pts{]} 3.2 Compare Logistic Regression and
KNN}\label{pts-3.2-compare-logistic-regression-and-knn}}

In your own words, describe the key differences between Logistic
Regression and KNN? When would you use one over the other?

    \textbf{Answer:} Logistic regression is a parametric linear model used
for binary classification. It estimates the probability that a given
input belongs to a particular class. This model assumes that there is a
linear relationship between the inputs and the outputs and might be
unsuitable for datasets where the inputs and outputs are related in a
non-linear fashion. Thus logistic regression derives a linear decision
boundary.

However, KNN is a non-parametric model used for classification and
regression. It classifies an input based on the majority class of its
k-nearest neighbors in the feature space. Since KNN is non-parametric,
it requires all datapoints to be present when a new sample point is
being predicted, so it can be computationally very complex in a high
dimensional feature space or with a large number of datapoints as
compared to logistic regression.

Generally, logistic regression can be used for classification problems
where the dataset can have a linear decision boundary, while KNN can be
used in situations where the decision boundary is not necessarily
linear. Logistic regression might be preferred for computational
efficiency as compared to KNN.

    \hypertarget{pts-3.3-tweaking-the-logistic-regression}{%
\subsubsection{{[}5 pts{]} 3.3 Tweaking the Logistic
Regression}\label{pts-3.3-tweaking-the-logistic-regression}}

\textbf{What are some parameters we can change that will affect the
performance of Logistic Regression?}

    \textbf{Answer:} The performance of Logistic Regression is influenced by
a number of parameters including the penalty type (L1 vs L2
regularization, i.e.~lasso vs ridge regularization), strength of
regularization (lambda parameter), class weights (for instances of
imbalanced datasets where one class is underrepresented) and different
types of solvers.

    \textbf{Implement Logistic Regression with the following specifications,
Report the test accuracy and plot the confusion matrix} - Use
\texttt{saga} solver (Stochastic Average Gradient Accelerated Method) -
L2 penalty (ridge regularization) - Max iteration = 1000 - \texttt{C} =
1 (inverse of regularization strength)

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{23}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}TODO: Your code goes here}
\PY{n}{new\PYZus{}log\PYZus{}reg} \PY{o}{=} \PY{n}{LogisticRegression}\PY{p}{(}\PY{n}{solver}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{saga}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{penalty}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{max\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{,} \PY{n}{C}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{new\PYZus{}log\PYZus{}reg}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}processed}\PY{p}{,} \PY{n}{train\PYZus{}target}\PY{p}{)}
\PY{n}{testing\PYZus{}result} \PY{o}{=} \PY{n}{new\PYZus{}log\PYZus{}reg}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{test\PYZus{}processed}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZpc{}\PYZhy{}12s}\PY{l+s+s2}{ }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{.}\PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{test\PYZus{}target}\PY{p}{,} \PY{n}{testing\PYZus{}result}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy:    0.819672
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{24}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{helper}\PY{o}{.}\PY{n}{draw\PYZus{}confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{test\PYZus{}target}\PY{p}{,} \PY{n}{testing\PYZus{}result}\PY{p}{,} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Not Sick (0)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Sick (1)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Project2_files/Project2_67_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \textbf{Now, Implement the same regression with \texttt{c=0.001}. Report
on the test accuracy and plot the confusion matrix.}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{25}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}TODO: Your code goes here}
\PY{n}{new\PYZus{}log\PYZus{}reg} \PY{o}{=} \PY{n}{LogisticRegression}\PY{p}{(}\PY{n}{solver}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{saga}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{penalty}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{max\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{,} \PY{n}{C}\PY{o}{=}\PY{l+m+mf}{0.001}\PY{p}{)}
\PY{n}{new\PYZus{}log\PYZus{}reg}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}processed}\PY{p}{,} \PY{n}{train\PYZus{}target}\PY{p}{)}
\PY{n}{testing\PYZus{}result} \PY{o}{=} \PY{n}{new\PYZus{}log\PYZus{}reg}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{test\PYZus{}processed}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZpc{}\PYZhy{}12s}\PY{l+s+s2}{ }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{.}\PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{test\PYZus{}target}\PY{p}{,} \PY{n}{testing\PYZus{}result}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy:    0.565574
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{26}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{helper}\PY{o}{.}\PY{n}{draw\PYZus{}confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{test\PYZus{}target}\PY{p}{,} \PY{n}{testing\PYZus{}result}\PY{p}{,} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Not Sick (0)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Sick (1)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Project2_files/Project2_70_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \textbf{Did the accuracy drop or improve? Why?}

    \textbf{Answer:} The accuracy decreases significantly from about 80\% to
approximately 55\% when the c value decreases from 1 to 0.001. The
decrease in the C value corresponds to an increase in the strength of
regularization. As seen in class, when the regularizer term is
prioritised more, it suppresses the actual loss function of the model by
penalizing, and makes the model less overfitted (more underfitted). As
the model becomes more underfitted, its accuracy decreases and this is
reflected by the test accuracies computed above.

    \hypertarget{pts-3.4-trying-out-different-penalties}{%
\subsubsection{{[}5 pts{]} 3.4 Trying out different
penalties}\label{pts-3.4-trying-out-different-penalties}}

\textbf{Now, implement Logistic Regression with the following
specifications, Report the test accuracy and plot the confusion matrix}
- Use \texttt{saga} solver (Stochastic Average Gradient Accelerated
Method) - L1 penalty (LASSO regularization) - Max iteration = 1000 -
\texttt{C} = 1 (inverse of regularization strength)

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{27}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}TODO: Your code goes here}
\PY{n}{new\PYZus{}log\PYZus{}reg} \PY{o}{=} \PY{n}{LogisticRegression}\PY{p}{(}\PY{n}{solver}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{saga}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{penalty}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{max\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{,} \PY{n}{C}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{new\PYZus{}log\PYZus{}reg}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}processed}\PY{p}{,} \PY{n}{train\PYZus{}target}\PY{p}{)}
\PY{n}{testing\PYZus{}result} \PY{o}{=} \PY{n}{new\PYZus{}log\PYZus{}reg}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{test\PYZus{}processed}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZpc{}\PYZhy{}12s}\PY{l+s+s2}{ }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Accuracy:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{.}\PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{test\PYZus{}target}\PY{p}{,} \PY{n}{testing\PYZus{}result}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy:    0.827869
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{28}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{helper}\PY{o}{.}\PY{n}{draw\PYZus{}confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{test\PYZus{}target}\PY{p}{,} \PY{n}{testing\PYZus{}result}\PY{p}{,} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Not Sick (0)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Sick (1)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Project2_files/Project2_75_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \textbf{Describe what the purpose of a penalty term is and how the
change from L2 to L1 affected the model.}

    \textbf{Answer:} TODO

    \hypertarget{pts-part-4.-additional-learning-methods-svm-support-vector-machine}{%
\subsection{{[}20 pts{]} Part 4. Additional Learning Methods: SVM
(Support Vector
Machine)}\label{pts-part-4.-additional-learning-methods-svm-support-vector-machine}}

    A Support Vector Machine (SVM) is a discriminative classifier formally
defined by a separating hyperplane. In other words, given labeled
training data (supervised learning), the algorithm outputs an optimal
hyperplane which categorizes new examples. In two dimensional space this
hyperplane is a line dividing a plane in two parts each corresponding to
one of the two classes.

Recall that \texttt{scikit-learn} uses \emph{soft-margin SVM} to account
for datasets that are not separable.

    \hypertarget{pts-4.1-run-default-svm-classifier}{%
\subsubsection{{[}5 pts{]} 4.1 Run default SVM
classifier}\label{pts-4.1-run-default-svm-classifier}}

Implement a Support Vector Machine classifier on your pipelined data.
Review the
\href{https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html}{SVM
Documentation} for how to implement a model. For this implementation you
can simply use the default settings. \textbf{Report on the test accuracy
and plot the confusion matrix.}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{29}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}TODO: Your code goes here}
\PY{n}{svm} \PY{o}{=} \PY{n}{SVC}\PY{p}{(}\PY{p}{)}
\PY{n}{svm}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}processed}\PY{p}{,} \PY{n}{train\PYZus{}target}\PY{p}{)}
\PY{n}{predicted} \PY{o}{=} \PY{n}{svm}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{test\PYZus{}processed}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZpc{}\PYZhy{}12s}\PY{l+s+s2}{ }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{.}\PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{test\PYZus{}target}\PY{p}{,}\PY{n}{predicted}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy:    0.803279
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{30}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{helper}\PY{o}{.}\PY{n}{draw\PYZus{}confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{test\PYZus{}target}\PY{p}{,} \PY{n}{predicted}\PY{p}{,} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Not Sick (0)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Sick (1)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Project2_files/Project2_82_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \textbf{Print out the number of support vectors that SVC has determined.
Look at the documentation for how to get this.}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{31}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}TODO: Your code goes here}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{svm}\PY{o}{.}\PY{n}{n\PYZus{}support\PYZus{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[54 52]
    \end{Verbatim}

    You may find that there are quite a few support vectors. This is due in
part to the small number of samples in the training set and the choice
of kernel.

    \hypertarget{pts-4.2-use-a-linear-svm}{%
\subsubsection{{[}5 pts{]} 4.2 Use a Linear
SVM}\label{pts-4.2-use-a-linear-svm}}

\texttt{SVC} defaults to use Gaussian kernel. \textbf{Now rerun your
SVM, but now use linear kernel. Report on the test accuracy and plot the
confusion matrix. Also, print out the number of support vectors.}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{32}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}TODO: Your code goes here}
\PY{n}{svm} \PY{o}{=} \PY{n}{SVC}\PY{p}{(}\PY{n}{kernel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{svm}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}processed}\PY{p}{,} \PY{n}{train\PYZus{}target}\PY{p}{)}
\PY{n}{predicted} \PY{o}{=} \PY{n}{svm}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{test\PYZus{}processed}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZpc{}\PYZhy{}12s}\PY{l+s+s2}{ }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{.}\PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{test\PYZus{}target}\PY{p}{,}\PY{n}{predicted}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy:    0.844262
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{33}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{helper}\PY{o}{.}\PY{n}{draw\PYZus{}confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{test\PYZus{}target}\PY{p}{,} \PY{n}{predicted}\PY{p}{,} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Not Sick (0)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Sick (1)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Project2_files/Project2_88_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{34}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{n}{svm}\PY{o}{.}\PY{n}{n\PYZus{}support\PYZus{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[34 31]
    \end{Verbatim}

    You will notice that number of support vectors has decreased
significantly.

    \hypertarget{pts-4.3-compare-gaussian-kernel-and-linear-kernel}{%
\subsubsection{{[}5 pts{]} 4.3 Compare Gaussian kernel and Linear
kernel}\label{pts-4.3-compare-gaussian-kernel-and-linear-kernel}}

Explain what the new results you've achieved mean. Read the
documentation to understand what you've changed about your model and
explain why changing that input parameter might impact the results in
the manner you've observed.

    \textbf{Answer:} TODO

    \hypertarget{pts-4.4-compare-svm-and-logistic-regression}{%
\subsubsection{{[}5 pts{]} 4.4 Compare SVM and Logistic
Regression}\label{pts-4.4-compare-svm-and-logistic-regression}}

Both logistic regression and linear SVM are trying to classify data
points using a linear decision boundary but achieve it in different
ways. In your own words, explain the difference between the ways that
Logistic Regression and Linear SVM find the boundary?

    \textbf{Answer:} Logistic regression and linear SVM are both linear
classifiers, i.e they seek to find a linear decision boundary. Logistic
regression does this by modelling a probability that an input belongs to
a particular class. This is done by finding a linear function that
estimates the probability distribution of a particular class. However,
linear SVM is non-probabilistic and aims to find the best separating
hyperplane (i.e.~the hyperplane that separates the data with the maximum
margin).

While Logistic Regression maximizes likelihood, linear SVM maximises the
margin between classes, focusing on a clear separation. Logistic
regression has a decision boundary where the probability of being in
either class is 0.5.

    \hypertarget{pts-part-5-cross-validation-and-model-selection}{%
\subsection{{[}10 pts{]} Part 5: Cross Validation and Model
Selection}\label{pts-part-5-cross-validation-and-model-selection}}

    You've sampled a number of different classification techniques and have
seen their performance on the dataset. Before we draw any conclusions on
which model is best, we want to ensure that our results are not the
result of the random sampling of our data we did with the
Train-Test-Split. To ensure otherwise we will conduct a K-Fold
Cross-Validation with GridSearch to determine which model perform best
and assess its performance on the test set.

    \hypertarget{pts-model-selection}{%
\subsubsection{{[}10 pts{]} Model Selection}\label{pts-model-selection}}

Run a \texttt{GridSearchCV} with 3-Fold Cross Validation. You will be
running each classification model with different parameters.

KNN: - \texttt{n\_neighbors\ =\ {[}1,3,5,7{]}} -
\texttt{metric\ =\ {[}"euclidean","manhattan"{]}\ \#Different\ Distance\ functions}

Logistic Regression: - \texttt{penalty\ =\ {[}"l1","l2"{]}} -
\texttt{solver\ =\ {[}"liblinear","saga"{]}} -
\texttt{C\ =\ {[}0.001,0.1,10{]}}

SVM: - \texttt{kernel\ =\ {[}"linear","rbf"{]}} -
\texttt{C\ =\ {[}0.001,0.1,10{]}}

Make sure to train and test your model on the transformed data and not
on the raw data.

\emph{Note: You may have to increase the number of iterations for
convergence for some of the models.}

After using \texttt{GridSearchCV}, put the results into a
\texttt{pandas\ Dataframe} and print out the whole table.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{35}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}TODO: Your code goes here}
\PY{n}{param\PYZus{}knn} \PY{o}{=} \PY{p}{[}
    \PY{p}{\PYZob{}}
      \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{n\PYZus{}neighbors}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{7}\PY{p}{]}\PY{p}{,}
      \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{metric}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{euclidean}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{manhattan}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,}
    \PY{p}{\PYZcb{}}
\PY{p}{]}

\PY{n}{param\PYZus{}log\PYZus{}reg} \PY{o}{=} \PY{p}{[}
    \PY{p}{\PYZob{}}
      \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{penalty}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{l1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{l2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,}
      \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{solver}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{liblinear}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{saga}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,}
      \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{C}\PY{l+s+s2}{\PYZdq{}} \PY{p}{:} \PY{p}{[}\PY{l+m+mf}{0.001}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{]}\PY{p}{,}
      \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{max\PYZus{}iter}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{1000}\PY{p}{]}
    \PY{p}{\PYZcb{}}
\PY{p}{]}

\PY{n}{param\PYZus{}svm} \PY{o}{=} \PY{p}{[}
    \PY{p}{\PYZob{}}
      \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{kernel}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{linear}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{rbf}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,}
      \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{C}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}\PY{p}{[}\PY{l+m+mf}{0.001}\PY{p}{,}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{]}\PY{p}{,}
    \PY{p}{\PYZcb{}}
\PY{p}{]}

\PY{n}{k} \PY{o}{=} \PY{l+m+mi}{3}
\PY{n}{kf} \PY{o}{=} \PY{n}{KFold}\PY{p}{(}\PY{n}{n\PYZus{}splits}\PY{o}{=}\PY{n}{k}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}

\PY{n}{knn} \PY{o}{=} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{p}{)}
\PY{n}{grid} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{knn}\PY{p}{,} \PY{n}{param\PYZus{}knn}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{n}{kf}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{accuracy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{grid}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}processed}\PY{p}{,} \PY{n}{train\PYZus{}target}\PY{p}{)}
\PY{n}{res} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{grid}\PY{o}{.}\PY{n}{cv\PYZus{}results\PYZus{}}\PY{p}{)}

\PY{n}{log\PYZus{}reg} \PY{o}{=} \PY{n}{LogisticRegression}\PY{p}{(}\PY{p}{)}
\PY{n}{grid} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{log\PYZus{}reg}\PY{p}{,} \PY{n}{param\PYZus{}log\PYZus{}reg}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{n}{kf}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{accuracy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{grid}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}processed}\PY{p}{,} \PY{n}{train\PYZus{}target}\PY{p}{)}
\PY{n}{res} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{res}\PY{p}{,}\PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{grid}\PY{o}{.}\PY{n}{cv\PYZus{}results\PYZus{}}\PY{p}{)}\PY{p}{]}\PY{p}{)}

\PY{n}{svm} \PY{o}{=} \PY{n}{SVC}\PY{p}{(}\PY{p}{)}
\PY{n}{grid} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{svm}\PY{p}{,} \PY{n}{param\PYZus{}svm}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{n}{kf}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{accuracy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{grid}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}processed}\PY{p}{,} \PY{n}{train\PYZus{}target}\PY{p}{)}
\PY{n}{res} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{res}\PY{p}{,} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{grid}\PY{o}{.}\PY{n}{cv\PYZus{}results\PYZus{}}\PY{p}{)}\PY{p}{]}\PY{p}{)}

\PY{n}{res}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{35}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
    mean\_fit\_time  std\_fit\_time  mean\_score\_time  std\_score\_time param\_metric  \textbackslash{}
0        0.001176      0.000263         0.006473        0.000757    euclidean
1        0.000954      0.000008         0.005926        0.000066    euclidean
2        0.000953      0.000038         0.007439        0.002003    euclidean
3        0.001044      0.000099         0.011297        0.007486    euclidean
4        0.000960      0.000017         0.005922        0.000089    manhattan
5        0.001089      0.000137         0.006576        0.000431    manhattan
6        0.001129      0.000040         0.007464        0.000503    manhattan
7        0.001081      0.000112         0.006467        0.000152    manhattan
0        0.001515      0.000084         0.000954        0.000064          NaN
1        0.002130      0.000759         0.001270        0.000210          NaN
2        0.002100      0.000054         0.001133        0.000072          NaN
3        0.003946      0.000504         0.001393        0.000131          NaN
4        0.001579      0.000455         0.000849        0.000053          NaN
5        0.005079      0.000999         0.001017        0.000137          NaN
6        0.001632      0.000287         0.001741        0.001211          NaN
7        0.006616      0.003567         0.001100        0.000111          NaN
8        0.002036      0.000421         0.000911        0.000012          NaN
9        0.063221      0.006629         0.001470        0.000105          NaN
10       0.001682      0.000128         0.000895        0.000036          NaN
11       0.025794      0.004904         0.001460        0.000082          NaN
0        0.001979      0.000099         0.001099        0.000034          NaN
1        0.002300      0.000369         0.001321        0.000035          NaN
2        0.001812      0.000298         0.001149        0.000326          NaN
3        0.002398      0.000076         0.001462        0.000053          NaN
4        0.010268      0.007269         0.001275        0.000223          NaN
5        0.003056      0.000742         0.001395        0.000127          NaN

   param\_n\_neighbors                                             params  \textbackslash{}
0                  1          \{'metric': 'euclidean', 'n\_neighbors': 1\}
1                  3          \{'metric': 'euclidean', 'n\_neighbors': 3\}
2                  5          \{'metric': 'euclidean', 'n\_neighbors': 5\}
3                  7          \{'metric': 'euclidean', 'n\_neighbors': 7\}
4                  1          \{'metric': 'manhattan', 'n\_neighbors': 1\}
5                  3          \{'metric': 'manhattan', 'n\_neighbors': 3\}
6                  5          \{'metric': 'manhattan', 'n\_neighbors': 5\}
7                  7          \{'metric': 'manhattan', 'n\_neighbors': 7\}
0                NaN  \{'C': 0.001, 'max\_iter': 1000, 'penalty': 'l1'{\ldots}
1                NaN  \{'C': 0.001, 'max\_iter': 1000, 'penalty': 'l1'{\ldots}
2                NaN  \{'C': 0.001, 'max\_iter': 1000, 'penalty': 'l2'{\ldots}
3                NaN  \{'C': 0.001, 'max\_iter': 1000, 'penalty': 'l2'{\ldots}
4                NaN  \{'C': 0.1, 'max\_iter': 1000, 'penalty': 'l1', {\ldots}
5                NaN  \{'C': 0.1, 'max\_iter': 1000, 'penalty': 'l1', {\ldots}
6                NaN  \{'C': 0.1, 'max\_iter': 1000, 'penalty': 'l2', {\ldots}
7                NaN  \{'C': 0.1, 'max\_iter': 1000, 'penalty': 'l2', {\ldots}
8                NaN  \{'C': 10, 'max\_iter': 1000, 'penalty': 'l1', '{\ldots}
9                NaN  \{'C': 10, 'max\_iter': 1000, 'penalty': 'l1', '{\ldots}
10               NaN  \{'C': 10, 'max\_iter': 1000, 'penalty': 'l2', '{\ldots}
11               NaN  \{'C': 10, 'max\_iter': 1000, 'penalty': 'l2', '{\ldots}
0                NaN                   \{'C': 0.001, 'kernel': 'linear'\}
1                NaN                      \{'C': 0.001, 'kernel': 'rbf'\}
2                NaN                     \{'C': 0.1, 'kernel': 'linear'\}
3                NaN                        \{'C': 0.1, 'kernel': 'rbf'\}
4                NaN                      \{'C': 10, 'kernel': 'linear'\}
5                NaN                         \{'C': 10, 'kernel': 'rbf'\}

    split0\_test\_score  split1\_test\_score  split2\_test\_score  mean\_test\_score  \textbackslash{}
0            0.737705           0.783333           0.783333         0.768124
1            0.803279           0.783333           0.900000         0.828871
2            0.770492           0.833333           0.883333         0.829053
3            0.754098           0.800000           0.883333         0.812477
4            0.737705           0.750000           0.800000         0.762568
5            0.803279           0.800000           0.883333         0.828871
6            0.786885           0.833333           0.866667         0.828962
7            0.770492           0.833333           0.866667         0.823497
0            0.459016           0.566667           0.616667         0.547450
1            0.459016           0.433333           0.616667         0.503005
2            0.737705           0.783333           0.833333         0.784791
3            0.459016           0.600000           0.866667         0.641894
4            0.754098           0.733333           0.833333         0.773588
5            0.737705           0.766667           0.850000         0.784791
6            0.803279           0.833333           0.933333         0.856648
7            0.803279           0.833333           0.933333         0.856648
8            0.786885           0.800000           0.866667         0.817851
9            0.786885           0.800000           0.866667         0.817851
10           0.786885           0.783333           0.883333         0.817851
11           0.786885           0.783333           0.883333         0.817851
0            0.459016           0.566667           0.616667         0.547450
1            0.459016           0.566667           0.616667         0.547450
2            0.836066           0.850000           0.900000         0.862022
3            0.573770           0.766667           0.866667         0.735701
4            0.770492           0.833333           0.900000         0.834608
5            0.754098           0.750000           0.850000         0.784699

    std\_test\_score  rank\_test\_score param\_C param\_max\_iter param\_penalty  \textbackslash{}
0         0.021509                7     NaN            NaN           NaN
1         0.050951                3     NaN            NaN           NaN
2         0.046167                1     NaN            NaN           NaN
3         0.053493                6     NaN            NaN           NaN
4         0.026940                8     NaN            NaN           NaN
5         0.038534                3     NaN            NaN           NaN
6         0.032717                2     NaN            NaN           NaN
7         0.039874                5     NaN            NaN           NaN
0         0.065779               11   0.001           1000            l1
1         0.081052               12   0.001           1000            l1
2         0.039054                7   0.001           1000            l2
3         0.169039               10   0.001           1000            l2
4         0.043088                9     0.1           1000            l1
5         0.047602                7     0.1           1000            l1
6         0.055595                1     0.1           1000            l2
7         0.055595                1     0.1           1000            l2
8         0.034931                3      10           1000            l1
9         0.034931                3      10           1000            l1
10        0.046326                5      10           1000            l2
11        0.046326                5      10           1000            l2
0         0.065779                5   0.001            NaN           NaN
1         0.065779                5   0.001            NaN           NaN
2         0.027451                1     0.1            NaN           NaN
3         0.121563                4     0.1            NaN           NaN
4         0.052879                2      10            NaN           NaN
5         0.046205                3      10            NaN           NaN

   param\_solver param\_kernel
0           NaN          NaN
1           NaN          NaN
2           NaN          NaN
3           NaN          NaN
4           NaN          NaN
5           NaN          NaN
6           NaN          NaN
7           NaN          NaN
0     liblinear          NaN
1          saga          NaN
2     liblinear          NaN
3          saga          NaN
4     liblinear          NaN
5          saga          NaN
6     liblinear          NaN
7          saga          NaN
8     liblinear          NaN
9          saga          NaN
10    liblinear          NaN
11         saga          NaN
0           NaN       linear
1           NaN          rbf
2           NaN       linear
3           NaN          rbf
4           NaN       linear
5           NaN          rbf
\end{Verbatim}
\end{tcolorbox}
        
    \textbf{What was the best model and what was it's score?}

Ans: SVM with parameters: \texttt{(C=0.1,\ kernel=linear)}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{36}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}TODO: Your code goes here}
\PY{n}{res\PYZus{}sorted} \PY{o}{=} \PY{n}{res}\PY{o}{.}\PY{n}{sort\PYZus{}values}\PY{p}{(}\PY{n}{by}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean\PYZus{}test\PYZus{}score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{ascending}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Parameters:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{res\PYZus{}sorted}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{params}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Score:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{res\PYZus{}sorted}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean\PYZus{}test\PYZus{}score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Parameters: \{'C': 0.1, 'kernel': 'linear'\}
Score: 0.8620218579234972
    \end{Verbatim}

    \textbf{Using the best model you have, report the test accuracy and plot
the confusion matrix}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{37}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}TODO: Your code goes here}
\PY{n}{best\PYZus{}model} \PY{o}{=} \PY{n}{SVC}\PY{p}{(}\PY{n}{C}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{n}{kernel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{best\PYZus{}model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{train\PYZus{}processed}\PY{p}{,} \PY{n}{train\PYZus{}target}\PY{p}{)}
\PY{n}{prediction} \PY{o}{=} \PY{n}{best\PYZus{}model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{test\PYZus{}processed}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZpc{}\PYZhy{}12s}\PY{l+s+s2}{ }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{metrics}\PY{o}{.}\PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{test\PYZus{}target}\PY{p}{,}\PY{n}{prediction}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy:    0.811475
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{38}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{helper}\PY{o}{.}\PY{n}{draw\PYZus{}confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{test\PYZus{}target}\PY{p}{,} \PY{n}{prediction}\PY{p}{,} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Not Sick (0)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Sick (1)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Project2_files/Project2_103_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{45}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{!}jupyter\PY{+w}{ }nbconvert\PY{+w}{ }\PYZhy{}\PYZhy{}to\PY{+w}{ }latex\PY{+w}{ }\PY{l+s+s1}{\PYZsq{}/content/drive/MyDrive/CM148\PYZus{}Project2/Project2.ipynb\PYZsq{}}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
[NbConvertApp] Converting notebook
/content/drive/MyDrive/CM148\_Project2/Project2.ipynb to latex
[NbConvertApp] Support files will be in Project2\_files/
[NbConvertApp] Making directory
/content/drive/MyDrive/CM148\_Project2/Project2\_files
[NbConvertApp] Making directory
/content/drive/MyDrive/CM148\_Project2/Project2\_files
[NbConvertApp] Making directory
/content/drive/MyDrive/CM148\_Project2/Project2\_files
[NbConvertApp] Making directory
/content/drive/MyDrive/CM148\_Project2/Project2\_files
[NbConvertApp] Making directory
/content/drive/MyDrive/CM148\_Project2/Project2\_files
[NbConvertApp] Making directory
/content/drive/MyDrive/CM148\_Project2/Project2\_files
[NbConvertApp] Making directory
/content/drive/MyDrive/CM148\_Project2/Project2\_files
[NbConvertApp] Making directory
/content/drive/MyDrive/CM148\_Project2/Project2\_files
[NbConvertApp] Making directory
/content/drive/MyDrive/CM148\_Project2/Project2\_files
[NbConvertApp] Making directory
/content/drive/MyDrive/CM148\_Project2/Project2\_files
[NbConvertApp] Making directory
/content/drive/MyDrive/CM148\_Project2/Project2\_files
[NbConvertApp] Making directory
/content/drive/MyDrive/CM148\_Project2/Project2\_files
[NbConvertApp] Writing 99759 bytes to
/content/drive/MyDrive/CM148\_Project2/Project2.tex
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]

\end{Verbatim}
\end{tcolorbox}


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
