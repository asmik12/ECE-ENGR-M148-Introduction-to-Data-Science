{"cells":[{"cell_type":"markdown","metadata":{"id":"YmJ-nYHrogyH"},"source":["# Project 2 - Binary Classification Comparative Methods"]},{"cell_type":"markdown","metadata":{"id":"yts6X17mogyN"},"source":["For this project we're going to attempt a binary classification of a dataset using multiple methods and compare results.\n","\n","Our goals for this project will be to introduce you to several of the most common classification techniques, how to perform them and tweek parameters to optimize outcomes, how to produce and interpret results, and compare performance. You will be asked to analyze your findings and provide explanations for observed performance.\n","\n","\n","<b><u>DEFINITIONS</b></u>\n","\n","\n","<b> Binary Classification:</b>\n","In this case a complex dataset has an added 'target' label with one of two options. Your learning algorithm will try to assign one of these labels to the data.\n","\n","<b> Supervised Learning:</b>\n","This data is fully supervised, which means it's been fully labeled and we can trust the veracity of the labeling."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J_l5gD3vogyP","colab":{"base_uri":"https://localhost:8080/"},"outputId":"00efcf78-04bb-4f9d-d594-5506f6776709"},"outputs":[{"output_type":"stream","name":"stderr","text":["ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"]},{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-20-82a6bb54ece1>\", line 36, in <cell line: 36>\n","    get_ipython().run_line_magic('cd', \"'/content/drive/MyDrive/CM148_Project2'\")\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2418, in run_line_magic\n","    result = fn(*args, **kwargs)\n","  File \"<decorator-gen-85>\", line 2, in cd\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/magic.py\", line 187, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py\", line 342, in cd\n","    oldcwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'OSError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-20-82a6bb54ece1>\", line 36, in <cell line: 36>\n","    get_ipython().run_line_magic('cd', \"'/content/drive/MyDrive/CM148_Project2'\")\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2418, in run_line_magic\n","    result = fn(*args, **kwargs)\n","  File \"<decorator-gen-85>\", line 2, in cd\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/magic.py\", line 187, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py\", line 342, in cd\n","    oldcwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'OSError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n","    if (await self.run_code(code, result,  async_=asy)):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n","    self.showtraceback(running_compiled_code=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-20-82a6bb54ece1>\", line 36, in <cell line: 36>\n","    get_ipython().run_line_magic('cd', \"'/content/drive/MyDrive/CM148_Project2'\")\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2418, in run_line_magic\n","    result = fn(*args, **kwargs)\n","  File \"<decorator-gen-85>\", line 2, in cd\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/magic.py\", line 187, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py\", line 342, in cd\n","    oldcwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'OSError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n","    if (await self.run_code(code, result,  async_=asy)):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n","    self.showtraceback(running_compiled_code=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n","    return runner(coro)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n","    coro.send(None)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n","    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3492, in run_ast_nodes\n","    self.showtraceback()\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n","    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n"]}],"source":["#Here are a set of libraries we imported to complete this assignment.\n","#Feel free to use these or equivalent libraries for your implementation\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import matplotlib.pyplot as plt # this is used for the plot the graph\n","import matplotlib\n","import os\n","import time\n","#Sklearn classes\n","from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\n","from sklearn import metrics\n","from sklearn.svm import SVC  #SVM classifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","from sklearn.metrics import confusion_matrix\n","import sklearn.metrics.cluster as smc\n","\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder, Normalizer, MinMaxScaler\n","from sklearn.compose import ColumnTransformer, make_column_transformer\n","\n","from matplotlib import pyplot as plt\n","import itertools\n","\n","%matplotlib inline\n","\n","#Sets random seed\n","import random\n","random.seed(42)\n","\n","#Mouting Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# import the provided helper functions\n","from helper import save_fig, draw_confusion_matrix, heatmap, make_meshgrid, plot_contours"]},{"cell_type":"code","source":["!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U_HvqHOe-qSK","executionInfo":{"status":"ok","timestamp":1715386232788,"user_tz":420,"elapsed":148,"user":{"displayName":"ASMI KAWATKAR","userId":"17634571373854173540"}},"outputId":"848ccfa3-9baa-4274-9b6f-33f81d15c1d3"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["/root\n"]}]},{"cell_type":"code","source":["import sys\n","print(sys.path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4t-hnV7g86uX","executionInfo":{"status":"ok","timestamp":1715385647931,"user_tz":420,"elapsed":12,"user":{"displayName":"ASMI KAWATKAR","userId":"17634571373854173540"}},"outputId":"a4f6d9d5-6391-49eb-ed6e-1be4e5bc8593"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["['/content', '/env/python', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.10/dist-packages/IPython/extensions', '/root/.ipython']\n"]}]},{"cell_type":"markdown","metadata":{"id":"cI3ZxAj0ogyR"},"source":["# Example Project\n","\n","In this part, we will go over how to perform a Binary classification task using a variety of models. We will provide examples of how to train and evaluate these models."]},{"cell_type":"markdown","metadata":{"id":"2PKF3yI4ogyS"},"source":["## Dataset Description\n","\n","Healthcare is an important industry that uses machine learning to aid doctors in diagnosing many different kinds of illnesses and diseases. For this example project, we will be using the [Breast Cancer Wisconsin Dataset](https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data) to determine whether a mass found in a body is benign or malignant.\n","\n","Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\n","\n","Feature Information:\n","\n","Column 1: ID number\n","\n","Column 2: Diagnosis (M = malignant, B = benign)\n","\n","Ten real-valued features are computed for each cell nucleus:\n","\n","    1. radius (mean of distances from center to points on the perimeter)\n","    2. texture (standard deviation of gray-scale values)\n","    3. perimeter\n","    4. area\n","    5. smoothness (local variation in radius lengths)\n","    6. compactness (perimeter^2 / area - 1.0)\n","    7. concavity (severity of concave portions of the contour)\n","    8. concave points (number of concave portions of the contour)\n","    9. symmetry\n","    10. fractal dimension (\"coastline approximation\" - 1)\n","\n","Due to the statistical nature of the test, we are not able to get exact measurements of the previous values. Instead, the dataset contains the mean and standard error of the real-valued features.\n","\n","Columns 3-12 present the mean of the measured values\n","\n","Columns 13-22 present the standard error of the measured values"]},{"cell_type":"markdown","metadata":{"id":"AtSzaHvfogyS"},"source":["## Load and Analyze the dataset"]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kwapYBpCvr85","executionInfo":{"status":"ok","timestamp":1715382208212,"user_tz":420,"elapsed":28446,"user":{"displayName":"ASMI KAWATKAR","userId":"17634571373854173540"}},"outputId":"814524cc-0285-4a0f-c26d-47375d679314"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C37omtlPogyT"},"outputs":[],"source":["#Load Data\n","data = pd.read_csv(/content/sample_data)"]},{"cell_type":"markdown","metadata":{"id":"Gog73yDpogyU"},"source":["Always look at your dataset after loading it. Use information from .describe and .info to learn more about the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SD4fhxZgogyU"},"outputs":[],"source":["data.head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n7DbBApiogyV"},"outputs":[],"source":["data.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1-L4Eu0-ogyV"},"outputs":[],"source":["data.info()"]},{"cell_type":"markdown","metadata":{"id":"Pps3LagBogyW"},"source":["While .info shows that every entry has 569 non-null and there are 569 entries, it is good to explicitly check for nulls."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o6X_r_piogyW"},"outputs":[],"source":["data.isnull().sum()"]},{"cell_type":"markdown","metadata":{"id":"6bcuuIXIogyW"},"source":["Awesome! No need for imputation!"]},{"cell_type":"markdown","metadata":{"id":"HtJaam0cogyX"},"source":["While we are looking at the dataset, we shall remove the \"id\" column."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0b51T7NSogyX"},"outputs":[],"source":["data = data.drop([\"id\"],axis= 1)"]},{"cell_type":"markdown","metadata":{"id":"9lFE_rE3ogyX"},"source":["### Looking at the target labels\n","\n","For this project, we wish to classify the diagnosis column."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TkpYph5CogyX"},"outputs":[],"source":["data[\"diagnosis\"]"]},{"cell_type":"markdown","metadata":{"id":"mUDaUeGSogyY"},"source":["We need to transform this column into numerical column so that we may use them in our models. To do this, we will employ the LabelEncoder to automatically transform all the target label."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r-v164PtogyY","colab":{"base_uri":"https://localhost:8080/","height":218},"executionInfo":{"status":"error","timestamp":1715384135423,"user_tz":420,"elapsed":1778,"user":{"displayName":"ASMI KAWATKAR","userId":"17634571373854173540"}},"outputId":"03d19677-fc51-4b7a-bbf1-03724dc7723e"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'data' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-408998d914f7>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'diagnosis'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'diagnosis'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sex'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"]}],"source":["from sklearn.preprocessing import LabelEncoder\n","le = LabelEncoder()\n","\n","data['diagnosis'] = le.fit_transform(data['diagnosis'])\n","print(le.classes_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z7MW7i-5ogyY"},"outputs":[],"source":["data['diagnosis']"]},{"cell_type":"markdown","metadata":{"id":"cK4zMG3kogyY"},"source":["### Let's look at a histogram of the full dataset.\n","\n","Its always good to get a global view of your datasets by looking at their histograms. You might see some interesting trends."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eK0J07waogyZ"},"outputs":[],"source":["data.hist(figsize = (20,15))\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"rPVi0QjCogyZ"},"source":["From the histograms, we can see some interesting trends. Possible observations:\n","\n","- Many of the _se columns indicate a heavy skewness towards low values and have large tails\n","- Many of the _mean columns look more Gaussian in shape\n","- There is a large disparity between the ranges of certain features. For example, radius mean can go from 0 to 25 while smoothness_mean is in the range [0.050,0.150]. This indicates we will have to normalize or standardize the features if the models are sensitive to such measures."]},{"cell_type":"markdown","metadata":{"id":"0Z7WIVSVogyZ"},"source":["### Looking at the correlation matrix to get an idea about which features are important"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J51GdKyDogyZ"},"outputs":[],"source":["correlations = data.corr()\n","columns = list(data)\n","# Creates the heatmap\n","heatmap(correlations.values, columns, columns, figsize=(20, 12), cmap=\"hsv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wKmwOKHAogyZ"},"outputs":[],"source":["# Let's specifically look at the correlations of our target feature\n","correlations[\"diagnosis\"].sort_values(ascending=False)"]},{"cell_type":"markdown","metadata":{"id":"szFEz2o4ogya"},"source":["We can see that there is a lot of correlation between the features and the target label. Thus, we can expect to learn something from the data"]},{"cell_type":"markdown","metadata":{"id":"E9aSgd_nogya"},"source":["### When doing classification, check if classes are heavily imbalanced.\n","\n","It is important that the dataset does not prefer one class over any others. Otherwise, it may bias the model to not learn the minority classes well.\n","\n","Lets use a histogram and count the number of elements in each class."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uyUJCuByogya"},"outputs":[],"source":["data['diagnosis'].hist(bins=2, figsize=(5,5))\n","data['diagnosis'].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"Ko06IEIKogya"},"source":["There is a bit of an imbalance which is something to keep in mind if we find that our models do not perform well on the minority classes. For our purposes, this imbalance is not big enough to be an issue so we will not perform balancing techniques for this dataset.\n","\n","Since the dataset is small though, we want to be careful when making training and testing splits to ensure that there is enough of each class for both splits. We will show how to perform this shortly."]},{"cell_type":"markdown","metadata":{"id":"jLCb1epdogya"},"source":["### Setting up the data\n","\n","Before starting any model training, we have to split up the target labels from our features."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lvel0qSEogyb"},"outputs":[],"source":["y = data[\"diagnosis\"]\n","x = data.drop([\"diagnosis\"],axis = 1)"]},{"cell_type":"markdown","metadata":{"id":"HR3vDT-5ogyb"},"source":["Now, we also split the data into training and testing data. To ensure that there is not an imbalance of classes in the training and testing set, we will use the stratify parameter in train_test_split to perform stratified sampling on the data (Recall from lecture how stratified sampling is performed)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bSYT4Fl7ogyb"},"outputs":[],"source":["train_raw, test_raw, target, target_test = train_test_split(x,y, test_size=0.2, stratify= y, random_state=0)"]},{"cell_type":"markdown","metadata":{"id":"fH15mF9Nogyc"},"source":["Note that we marked the input feature data as raw to indicate that there has been no pre-processing on them such as standardization. Shortly, we will show the affect that pre-processing has on the performance of the model.\n","\n","Let us quickly test that the splits are somewhat balanced."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FWgw3lIGogyc"},"outputs":[],"source":["#Training classes\n","target.hist(bins=2, figsize=(5,5))\n","target.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RcTRA-tRogyc"},"outputs":[],"source":["#Testing classes\n","target_test.hist(bins=2, figsize=(5,5))\n","target_test.value_counts()"]},{"cell_type":"markdown","metadata":{"id":"fVf25piYogyc"},"source":["We can see that the class balance is about the same as before the split. In fact, we can see that if a classifier just guessed class 0, it would have an accuracy of $100 \\times \\frac{72}{72+42} = 63.15\\%$. We can consider this the minimum accuracy for your model to compare against."]},{"cell_type":"markdown","metadata":{"id":"UY-gH-lWogyi"},"source":["## Models for Classification: KNN"]},{"cell_type":"markdown","metadata":{"id":"kc9d0REmogyi"},"source":["For our first model, we will use KNN classfication. This is a model we have seen many times throughout the course and it would be interesting to see how well it performs."]},{"cell_type":"markdown","metadata":{"id":"bUZI1sV7ogyj"},"source":["### Simple KNN classification with K = 3\n","\n","Let us try KNN on the raw data with simply 3 nearest neighbors. We use the sklearn [metric library](https://scikit-learn.org/stable/modules/model_evaluation.html) to calculate the measures of interest. In this case, we focus on accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k-_ZBO86ogyj"},"outputs":[],"source":["# k-Nearest Neighbors algorithm\n","knn = KNeighborsClassifier(n_neighbors=3)\n","knn.fit(train_raw, target)\n","predicted = knn.predict(test_raw)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IPiBFqY_ogyj"},"outputs":[],"source":["print(\"%-12s %f\" % ('Accuracy:', metrics.accuracy_score(target_test,predicted)))"]},{"cell_type":"markdown","metadata":{"id":"QehDCWL6ogyj"},"source":["We can see that there is already a huge improvement in accuracy on comparison to the baseline of 63.15%. Let's see the effect that standardizing the input features would have on the KNN performance."]},{"cell_type":"markdown","metadata":{"id":"r7rZLhwFogyk"},"source":["### Affect of pre-processing on KNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WLDKoOmqogyk"},"outputs":[],"source":["# Since all features are real-valued, we only have one pipeline\n","pipeline = Pipeline([(\"scaler\", StandardScaler())])\n","\n","# Transform raw data. Note that we don't want to fit the test data\n","train = pipeline.fit_transform(train_raw)\n","test = pipeline.transform(test_raw)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hd75AnSQogyk"},"outputs":[],"source":["# k-Nearest Neighbors algorithm\n","knn = KNeighborsClassifier(n_neighbors=3)\n","knn.fit(train, target)\n","testing_result = knn.predict(test)\n","predicted = knn.predict(test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O84lZiJHogyk"},"outputs":[],"source":["print(\"%-12s %f\" % ('Accuracy:', metrics.accuracy_score(target_test,predicted)))"]},{"cell_type":"markdown","metadata":{"id":"32bPUpZPogyk"},"source":["We can see that with pre-processing we were able to get a much better classification accuracy.\n","\n","Here we only used StandardScaler. Lets see if other pre-processing techniques could have also worked. As such, lets  look at MinMaxScaler and Normalizer:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qSAP2XJ0ogyl"},"outputs":[],"source":["preprocessors = [StandardScaler(),MinMaxScaler(),Normalizer() ]\n","\n","for pre in preprocessors:\n","    pipeline = Pipeline([\n","        ('preprocessor', pre)\n","    ])\n","\n","\n","    #Transform raw data\n","    train = pipeline.fit_transform(train_raw)\n","    test = pipeline.transform(test_raw) #Note that there is no fit calls\n","    # k-Nearest Neighbors algorithm\n","    knn = KNeighborsClassifier(n_neighbors=7)\n","    knn.fit(train, target)\n","    testing_result = knn.predict(test)\n","    predicted = knn.predict(test)\n","\n","    print(pre)\n","    print(\"%-12s %f\" % ('Accuracy:', metrics.accuracy_score(target_test,predicted)))"]},{"cell_type":"markdown","metadata":{"id":"vH-wnbRFogyl"},"source":["We can see that MinMaxScaler had the same performance as StandardScaler. Yet, Normalizer did not improve the model."]},{"cell_type":"markdown","metadata":{"id":"upyhk3Chogyl"},"source":["### Visualizing decision boundaries  for KNN\n","\n","Its always nice to see the decision boundaries a model decides upon. Let's see how the decision boundary changes as function of k when only using the two most correlated features to the target labels: concave_points_mean and perimeter_mean."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"02jDd8E1ogyl"},"outputs":[],"source":["# Extract first two features and use the standardscaler\n","train_2 = StandardScaler().fit_transform(\n","    train_raw[[\"concave points_mean\", \"perimeter_mean\"]]\n",")\n","\n","k_r = [1, 3, 5, 7]\n","for k in k_r:\n","    knn = KNeighborsClassifier(n_neighbors=k)\n","    knn.fit(train_2, target)\n","    draw_contour(train_2, target, knn, class_labels=[\"Benign\", \"Malignant\"])\n","    plt.title(f\"K ={k}\")"]},{"cell_type":"markdown","metadata":{"id":"zjJTWwbuogym"},"source":["We can see that as k gets larger, the decision boundary gets smoother."]},{"cell_type":"markdown","metadata":{"id":"9K7XIrWTogym"},"source":["## Models for Classification: Logistic Regression\n","\n","While KNN is a very powerful model, it does come with a few issues such as\n","\n","- Require storing the full training dataset\n","- Prediction is done by comparing new sample will all samples in training set which is time-consuming\n","\n","These issues arise because KNN is a **non-parametric** model which means that it does not summarize the data into a finite set of parameters.\n","\n","Let us now look at Logistic Regression which is an example of a **parametric** model."]},{"cell_type":"markdown","metadata":{"id":"gdPBw91dogym"},"source":["### Simple Logistic Regression\n"]},{"cell_type":"markdown","metadata":{"id":"ZaE0EtpEogyn"},"source":["First, let us see how logistic regression performs without any regularization."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8F7e95G2ogyn"},"outputs":[],"source":["log_reg = LogisticRegression(penalty=\"l2\", max_iter=10000, solver=\"lbfgs\", C=10**30)\n","# C is choosen to be high to remove regularization\n","# We could have chosen penalty = \"none\" since lbfgs supports it but this option is not possible for all solvers.\n","\n","log_reg.fit(train_raw, target)\n","testing_result = log_reg.predict(test_raw)\n","predicted = log_reg.predict(test_raw)\n","\n","print(\"%-12s %f\" % (\"Accuracy:\", metrics.accuracy_score(target_test, predicted)))"]},{"cell_type":"markdown","metadata":{"id":"IGZyauVPogyn"},"source":["We can see that Logistic Regression is actually performing much better than any of the KNN models we tried. We can also see the parameters that the model learned."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_QNrYR0gogyn"},"outputs":[],"source":["#Parameters for each feature\n","log_reg.coef_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FEIZAYTcogyo"},"outputs":[],"source":["#Intercept term\n","log_reg.intercept_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"heqqXU7mogyo"},"outputs":[],"source":["print(\"Number of Features in data:\", train_raw.shape[1])\n","print(\"Number of Parameters:\", len(log_reg.coef_[0]))"]},{"cell_type":"markdown","metadata":{"id":"qSfTh0ivogyo"},"source":["Since we are using Logistic Regression where we model the log odds with a linear function, it makes sense that we have a parameter/coefficient for each input feature."]},{"cell_type":"markdown","metadata":{"id":"4yi73JJQogyo"},"source":["###  Parameters for Logistic Regression\n","\n","In Sci-kit Learn, the following are just some of the parameters we can pass into Logistic Regression:\n","\n","- penalty: {'l1', 'l2', 'elasticnet’, 'none’} default=\"l2\"\n","    - Specifies the type of regularization to use. Not all penalties work for each solver.\n","- C: positive float, default=1\n","    - Inverse of the regularization strength. You can treat C as $\\frac{1}{\\lambda}$ as shown in lecture. Thus, as C gets smaller, the regularization strength increases.\n","- solver: {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'}, default=’lbfgs’\n","    - Algorithm to use in the optimization problem. Each algorithms solves logistic regression using different iterative methods that are based on the gradient. Read the [sci-kit learn documentation](https://scikit-learn.org/dev/modules/linear_model.html#logistic-regression) for more information.\n","- max_iter: int, default=100\n","    - Maximum number of iterations taken for the solvers to converge."]},{"cell_type":"markdown","metadata":{"id":"FLzoruTiogyp"},"source":["Each parameter has a different effect on the model. Let's look at how the choose of max_iter affects the model performance on the raw data and the standardized dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5jdn2wqdogyp"},"outputs":[],"source":["#Since all features are real-valued, we only have one pipeline\n","preprocesser = Pipeline([\n","    ('scaler', StandardScaler())\n","])\n","\n","\n","#Transform raw data\n","train = preprocesser.fit_transform(train_raw)\n","test = preprocesser.transform(test_raw) #Note that there is no fit call"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R0lrdWjtogyp"},"outputs":[],"source":["log_reg = LogisticRegression(penalty = \"l2\",max_iter = 2000, solver = \"lbfgs\", C= 0.01)\n","\n","#Train raw is the data before preprocessing\n","log_reg.fit(train_raw, target)\n","predicted = log_reg.predict(test_raw)\n","print(\"%-12s %f\" % ('Raw Data Accuracy:', metrics.accuracy_score(target_test,predicted)))\n","\n","\n","#Train is the data after preprocessing (using Standard scalar)\n","log_reg.fit(train, target)\n","predicted = log_reg.predict(test)\n","\n","print(\"%-12s %f\" % ('Preprocessed Data Accuracy:', metrics.accuracy_score(target_test,predicted)))"]},{"cell_type":"markdown","metadata":{"id":"NnWBoQopogyq"},"source":["We see that the accuraccies are pretty close to each other.  Lets see what happens when we decrease the max_iter."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mNvsy0_7ogyq"},"outputs":[],"source":["log_reg = LogisticRegression(penalty=\"l2\", max_iter=70, solver=\"lbfgs\", C=0.01)\n","\n","# Train raw is the data before preprocessing\n","log_reg.fit(train_raw, target)\n","predicted = log_reg.predict(test_raw)\n","print(\n","    \"%-12s %f\" % (\"Raw Data Accuracy:\", metrics.accuracy_score(target_test, predicted))\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ulOrCjBWogyq"},"outputs":[],"source":["# Train is the data after preprocessing (using Standard scalar)\n","log_reg.fit(train, target)\n","predicted = log_reg.predict(test)\n","\n","print(\n","    \"%-12s %f\"\n","    % (\"Preprocessed Data Accuracy:\", metrics.accuracy_score(target_test, predicted))\n",")"]},{"cell_type":"markdown","metadata":{"id":"TIbJkh5Wogyr"},"source":["Ooops! The model did not seem to converge. Its seem that the scale of the features strongly affects the convergence speed of the iterative algorithm. As suggested, we can fix this issue by increaing the max_iter, re-scaling the data, or using a different solver.\n"]},{"cell_type":"markdown","metadata":{"id":"B_VKOTLiogyr"},"source":["### Cross Validation for Logistic Regression"]},{"cell_type":"markdown","metadata":{"id":"xa0klnaWogyr"},"source":["Let us do a little experiment using cross validation to see how each term affects the logistic regression. We will perform this example on the standardized data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0oTwZGFHogyr"},"outputs":[],"source":["# You may even do Cross validation for classification\n","\n","from sklearn.model_selection import GridSearchCV\n","\n","# Note that this a list of dict\n","# Each dict describes the combination of parameters to check\n","parameters = [\n","    {\n","        \"penalty\": [\"l2\"],\n","        \"C\": [0.01, 1, 100],\n","        \"solver\": [\"lbfgs\", \"liblinear\"],\n","    },  # These solvers support penalty = \"l2\"\n","    {\n","        \"penalty\": [None],\n","        \"C\": [1],  # Specified to prevent error message\n","        \"solver\": [\"lbfgs\", \"newton-cg\"],  # These solvers support penalty = \"none\"\n","    },\n","]\n","\n","# instantiate model\n","\n","# Implementing cross validation\n","k = 3\n","kf = KFold(n_splits=k, random_state=None)\n","\n","# will change parameters during CV\n","log_reg = LogisticRegression(penalty=\"none\", max_iter=1000, solver=\"lbfgs\")\n","grid = GridSearchCV(log_reg, parameters, cv=kf, scoring=\"accuracy\")\n","grid.fit(train, target)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5t2ChFeAogyr"},"outputs":[],"source":["#Put results into Dataframe\n","res= pd.DataFrame(grid.cv_results_)\n","res"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KsjLmKLLogys"},"outputs":[],"source":["# Extract the columns that specify the score and the parameters for each row\n","res[[\"rank_test_score\", \"param_C\", \"param_penalty\", \"param_solver\", \"mean_test_score\"]]"]},{"cell_type":"markdown","metadata":{"id":"MTj5sD-4ogys"},"source":["We can see that the choice of these parameters can stronlgy affect performance of the classifier. Lets check the performance of the best parameters on the test set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GUb2czhKogys"},"outputs":[],"source":["#Train raw is the data before preprocessing\n","predicted = grid.predict(test)\n","print(\"%-12s %f\" % ('Accuracy:', metrics.accuracy_score(target_test,predicted)))"]},{"cell_type":"markdown","metadata":{"id":"quMlro9bogyt"},"source":["Note that this test accuracy is not as good as some of the other logistic regression examples we've shown."]},{"cell_type":"markdown","metadata":{"id":"J3JL6qkoogyt"},"source":["### Speedtest between KNN and Logistic Regression\n","\n","Lets see how long KNN and Logistic Regression take to perform training and testing.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QqowFCQSogyt"},"outputs":[],"source":["scaler = StandardScaler()\n","train = scaler.fit_transform(train_raw)\n","test = scaler.fit_transform(test_raw)\n","\n","\n","log_reg = LogisticRegression(penalty=None, max_iter=1000)\n","knn = KNeighborsClassifier(n_neighbors=3)\n","\n","t0 = time.time()\n","knn.fit(train, target)\n","t1 = time.time()\n","print(\"KNN Training Time : \", t1 - t0)\n","\n","t0 = time.time()\n","log_reg.fit(train, target)\n","t1 = time.time()\n","print(\"Logistic Regression Training Time : \", t1 - t0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2iPUuzjtogyt"},"outputs":[],"source":["t0 = time.time()\n","knn.predict(test)\n","t1 = time.time()\n","print(\"KNN Testing Time : \", t1-t0)\n","\n","t0 = time.time()\n","log_reg.predict(test)\n","t1 = time.time()\n","print(\"Logistic Regression Testing Time : \", t1-t0)"]},{"cell_type":"markdown","metadata":{"id":"550wYha3ogyu"},"source":["This simple test shows that Logistic Regression is slower than KNN during Training time but is much faster during testing time."]},{"cell_type":"markdown","metadata":{"id":"6STZcIPoogyu"},"source":["### Visualizing decision boundaries for Logistic Regression\n","\n","Now, lets look at the decision boundary caused by Logistic Regression. Same as for KNN, we use the two most correlated features to the target labels: concave_points_mean and perimeter_mean. This way, we can visualize the 2D decision boundary."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GTLkMukkogyu"},"outputs":[],"source":["#Extract first two feature and use the standardscaler\n","train_2 = StandardScaler().fit_transform(train_raw[['concave points_mean','perimeter_mean']])\n","\n","Cs  = [0.001,0.1,1000]\n","for C in Cs:\n","    log_reg = LogisticRegression(penalty = \"l2\",max_iter = 1000, solver = \"lbfgs\", C=C) #will change parameters during CV\n","    log_reg.fit(train_2, target)\n","\n","    draw_contour(train_2,target,log_reg, class_labels = ['Benign', 'Malignant'])\n","    plt.title(f\"C ={C}\")"]},{"cell_type":"markdown","metadata":{"id":"rdCl7y86ogyu"},"source":["We can see as the regularization strength changes, the decision boundary moves as well. Additionally, we can clearly see that the decision boundary is a line since this is a linear model."]},{"cell_type":"markdown","metadata":{"id":"ltH_9XkXogyv"},"source":["## Models for Classification: SVM\n","\n","We now discuss another type of linear classification model known as Support Vector Machines (SVM). Where Logistic Regression was motivated probability theory, SVM is motivated by geometeric arguments. Specifcally, SVM finds a separating hyperline that maximizes the margin (i.e. distance from each class). The hyperplane is used to classify the points by designating every sample on of side of the hyperplane as the positive class and the other side as the negative class.\n","\n","The hyperplane is determine by a few sample points known as support vectors that uniquely characterize the hyperplane.\n","\n","\n","![svm_im](images/support-vector-machine-algorithm.png)\n","\n","\n","Note that it may not always be possible to find a hyperplane that completely separates the classes. Thus, we use what is known as Soft-Margin SVM which aims to maximize the margin while minizming the distance on the classes that are on the wrong side.\n","\n","All Sci-kit learn implementations of SVM that we use are soft-margin SVM."]},{"cell_type":"markdown","metadata":{"id":"_6uyGsZdogyv"},"source":["### Simple SVM classification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l5Ob2Re8ogyv"},"outputs":[],"source":["svm = SVC()\n","svm.fit(train, target)\n","predicted = svm.predict(test)\n","print(\"%-12s %f\" % ('Accuracy:', metrics.accuracy_score(target_test,predicted)))"]},{"cell_type":"markdown","metadata":{"id":"F7hEdL3Fogyv"},"source":["###  Parameters for SVM\n","\n","In Sci-kit Learn, the following are just some of the parameters we can pass into Logistic Regression:\n","\n","- C: positive float, default=1\n","    - Inverse of the regularization strength. You can treat C as $\\frac{1}{\\lambda}$ as shown in lecture. Thus, as C gets smaller, the regularization strength increases. SVM only uses the L2 regularization.\n","- kernel: {'linear’, 'poly’, 'rbf’, 'sigmoid’}, default=’rbf’\n","    - Specifies the kernel type to be used in the algorithm. A kernel specifies a mapping into a higher dimension space to allow for non-linear decision boundaries.\n","- degree: int, default=3\n","    - Degree of the polynomial kernel function ('poly’). Ignored by all other kernels.\n","    "]},{"cell_type":"markdown","metadata":{"id":"zIIANQzcogyw"},"source":["### Visualizing decision boundaries for SVM\n","\n","Now, lets look at the decision boundary caused by SVM with different kernels. Same as for KNN and Logistic Regression, we use the two most correlated features to the target labels: concave_points_mean and perimeter_mean. This way, we can visualize the 2D decision boundary.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a7M31SUrogyw"},"outputs":[],"source":["#Extract first two feature and use the standardscaler\n","train_2 = StandardScaler().fit_transform(train_raw[['concave points_mean','perimeter_mean']])\n","\n","kernel  = ['linear', 'poly', 'rbf',  'sigmoid']\n","for ker in kernel:\n","    svm = SVC(kernel = ker) #will change parameters during CV\n","    svm.fit(train_2, target)\n","    draw_contour(train_2,target,svm,class_labels = ['Benign', 'Malignant'])\n","\n","    plt.title(f\"Kernel ={ker}\")"]},{"cell_type":"markdown","metadata":{"id":"mtIxtqchogyw"},"source":["We can see that the decision boundary is not always linear because we are using non-linear kernels."]},{"cell_type":"markdown","metadata":{"id":"-v60-4gOogyw"},"source":["## Important Measures for Classifications\n","\n","Now that we have gone over a few models for binary classification, let's explore the different ways we can measure the performance of these models."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CSNYNGUeogyx"},"outputs":[],"source":["#Example classifier\n","log_reg = LogisticRegression(max_iter = 1000)\n","log_reg.fit(train_raw, target)\n","predicted = log_reg.predict(test_raw)"]},{"cell_type":"markdown","metadata":{"id":"ivQ6kc2Iogyx"},"source":["Here are just some of the most important measures of interest. We use the convention to refer to the class labeled as $1$ as the positive class.\n","\n","- **Accuracy:** The percentage of predictions that are correct. Use metrics.accuracy_score\n","- **Precision:** $\\frac{\\text{Number of labels correctly classified as positive}}{\\text{Number of labels classified as positives}}$. Percentage of predictions that are correctly positive among all the predictions that were classified as positive. Use metrics.precision_score\n","- **Recall:** $\\frac{\\text{Number of labels correctly classified as positive}}{\\text{Number of labels where the true class is positive}}$. Percentage of predictions that are correctly positive among all the labels where the true class is positive. Also known as the probability of detecting when a class is positive. Use metrics.recall_score\n","- **F1 Score:** Harmonic mean of the precision and recall. Highest value is $1$ when both precision and recall are $1$, i.e. perfect. Lowest value is $0$ when either precision or recall is zero. Provides an aggregate score to analyze both precision and recall. Use metrics.f1_score\n","\n","We can calculate these measures by using a confusion matrix as well."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nLjae51vogyx"},"outputs":[],"source":["print(\"%-12s %f\" % ('Accuracy:', metrics.accuracy_score(target_test,predicted)))\n","print(\"%-12s %f\" % ('Precision:', metrics.precision_score(target_test,predicted, labels=None, pos_label=1, average='binary', sample_weight=None)))\n","print(\"%-12s %f\" % ('Recall:', metrics.recall_score(target_test,predicted, labels=None, pos_label=1, average='binary', sample_weight=None)))\n","print(\"%-12s %f\" % ('F1 Score:', metrics.f1_score(target_test,predicted, labels=None, pos_label=1, average='binary', sample_weight=None)))\n","print(\"Confusion Matrix: \\n\", metrics.confusion_matrix(target_test,predicted))\n","\n","#Draws confusion matrix\n","draw_confusion_matrix(target_test, predicted, ['Benign', 'Malignant'])"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"398.016px"},"toc_section_display":true,"toc_window_display":true},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}